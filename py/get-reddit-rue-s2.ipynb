{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Reddit Posts on Euphoria & Rue**\n",
    "07/21/2022\n",
    "\n",
    "*Season 2: Jan 9 -Feb 7, 2022*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reason: replicate upitt qualitative paper - same sampling criteria - more recent season.  \n",
    "hope to have more relvant topics but similar topics they determined thematically  \n",
    "sampling criteria:  \n",
    "- overall r/television subr\n",
    "- time frame  \n",
    "- comments that mention 'rue' - as narrator and main character  \n",
    "\n",
    "analysis: \n",
    "- topic modeling season 1 v 2\n",
    "  - LDA\n",
    "    - so far: getting comments for three recent top posts - 4 topics with 100 passes, no clear topics\n",
    "    - hyperparameter tuning\n",
    "      - coherence score/perplexity ...meh\n",
    "  - transformer-based\n",
    "    - `BERTopic`\n",
    "      - embeddings: \n",
    "        - all-mpnet-base-v2\n",
    "        - all-distilroberta-v1\n",
    "        - all-MiniLM-L6-v2 - *default*\n",
    "- compute valence scores for each comment and group by topic\n",
    "  - read into mood shift of main character from season 1 to 2\n",
    "- compare S1 to S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "from praw import Reddit\n",
    "from praw.models import MoreComments\n",
    "from praw.models import Comment\n",
    "from praw.models import Submission\n",
    "import datetime as dt\n",
    "from psaw import PushshiftAPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source api credentials from file\n",
    "file = 'api-creds.py'\n",
    "exec(open(file).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get posts headlines. Options: hot, new, top, or rising  \n",
    "\n",
    "- `score`: the number of votes\n",
    "- `created_etc`: date post of created\n",
    "- `upvote ratio`: ratio of post upvotes to commennts\n",
    "- `num_comments`: number of comments the post received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt solution from https://stackoverflow.com/questions/70068133/what-is-the-best-way-to-fetch-all-data-posts-and-their-comments-from-reddit\n",
    "\n",
    "# api = PushshiftAPI(reddit)\n",
    "\n",
    "# start_epoch = int(dt.datetime(2022, 1, 9).timestamp())\n",
    "# submission_generator = api.search_submissions(after=start_epoch, subreddit='euphoria',\n",
    "#                                               filter=['url', 'author',\n",
    "#                                                       'title', 'subreddit'],\n",
    "#                                               limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions = list(submission_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulted in 32,285 posts since Jan 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Submission(id='w4oz0h'),\n",
       " Submission(id='w4go60'),\n",
       " Submission(id='w4fbl2'),\n",
       " Submission(id='w49wy8'),\n",
       " Submission(id='w47n0l'),\n",
       " Submission(id='w46ply'),\n",
       " Submission(id='w44wju'),\n",
       " Submission(id='w44d7h'),\n",
       " Submission(id='w432m9')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submissions1[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions to dataframe\n",
    "# submissions_df = pd.DataFrame(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions_df.to_pickle('../dat/submission_ids.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Iterate over submission ids to get comments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comments for a submission\n",
    "\n",
    "# submission id = 'w4oz0h'\n",
    "\n",
    "# test = []\n",
    "# sub_id = 'w4oz0h'\n",
    "# for comment in reddit.submission(id=sub_id).comments:\n",
    "#     test.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the comments from all the posts in a subreddit\n",
    "# first get all the posts in a subreddit\n",
    "# then for each post get all the comments\n",
    "\n",
    "# def get_headlines(subreddit):\n",
    "#     \"\"\"\n",
    "#     Get ALL headlines from the 'Euphoria' subreddit\n",
    "#     \"\"\"\n",
    "#     headlines = []\n",
    "#     for submission in subreddit(subreddit):\n",
    "#         headlines.append({'title': submission.title, 'selftext': submission.selftext, 'url': submission.url, 'id': submission.id, 'author': submission.author, 'score': submission.score, 'created_utc': submission.created_utc, 'upvote_ratio': submission.upvote_ratio, 'num_comments': submission.num_comments})\n",
    "#     return headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get comments from the selected posts  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column\n",
    "# submissions_df.rename(columns={0: 'submission_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_comments = []\n",
    "# for post_id in submissions_df.submission_id:\n",
    "#     submission = reddit.submission(id=post_id)\n",
    "#     submission.comments.replace_more(limit=0)\n",
    "#     for comment in submission.comments.list():\n",
    "#         test_comments.append(comment)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_comments[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_comments_df = pd.DataFrame(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_comments_df.to_csv('../dat/test_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test pushift api again based on comments from redditor: https://www.reddit.com/r/pushshift/comments/w64ead/can_we_get_submissions_based_on_the_frequency_of/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    }
   ],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# inclusive, make this the first day to collect yyyy,m,d\n",
    "after = int(datetime(2022, 1, 9).timestamp()) -1\n",
    "\n",
    "# exclusive, make this the day after the last day to collect\n",
    "before = int(datetime(2022, 2, 8).timestamp())\n",
    "\n",
    "api=PushshiftAPI()\n",
    "\n",
    "# change to search_comments to collect comments instead of submissions\n",
    "subs = api.search_submissions(after=after, before=before, q=\"rue\", limit=None)\n",
    "\n",
    "# process however you want - this loads the posts into a dataframe\n",
    "df = pd.DataFrame([sub.d_ for sub in subs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../dat/rue_submissions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'author',\n",
       "       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n",
       "       'author_flair_type', 'author_fullname', 'author_is_blocked',\n",
       "       'author_patreon_flair', 'author_premium', 'awarders', 'can_mod_post',\n",
       "       'contest_mode', 'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
       "       'is_created_from_ads_ui', 'is_crosspostable', 'is_meta',\n",
       "       'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable',\n",
       "       'is_self', 'is_video', 'link_flair_background_color',\n",
       "       'link_flair_richtext', 'link_flair_text_color', 'link_flair_type',\n",
       "       'locked', 'media_only', 'no_follow', 'num_comments', 'num_crossposts',\n",
       "       'over_18', 'permalink', 'pinned', 'retrieved_on', 'score', 'selftext',\n",
       "       'send_replies', 'spoiler', 'stickied', 'subreddit', 'subreddit_id',\n",
       "       'subreddit_subscribers', 'subreddit_type', 'suggested_sort',\n",
       "       'thumbnail', 'title', 'total_awards_received', 'treatment_tags',\n",
       "       'upvote_ratio', 'url', 'created', 'link_flair_template_id',\n",
       "       'link_flair_text', 'parent_whitelist_status', 'pwls',\n",
       "       'whitelist_status', 'wls', 'removed_by_category',\n",
       "       'link_flair_css_class', 'author_flair_background_color',\n",
       "       'author_flair_text_color', 'banned_by', 'media', 'post_hint', 'preview',\n",
       "       'secure_media', 'thumbnail_height', 'thumbnail_width',\n",
       "       'url_overridden_by_dest', 'author_flair_template_id',\n",
       "       'crosspost_parent', 'crosspost_parent_list', 'gallery_data',\n",
       "       'is_gallery', 'media_metadata', 'media_embed', 'secure_media_embed',\n",
       "       'poll_data', 'author_cakeday', 'content_categories', 'collections',\n",
       "       'discussion_type', 'distinguished', 'event_end', 'event_is_live',\n",
       "       'event_start'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_pids_df = df[['id', 'title', 'selftext', 'url', 'author', 'score', 'created_utc', 'num_comments']]\n",
    "# convert created_utc to datetime\n",
    "s2_pids_df['created_utc_real'] = pd.to_datetime(s2_pids_df['created_utc'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like there a lot of non-relevant posts\n",
    "df['title'][0]\n",
    "# filter for domain in self.euphoria\n",
    "s2_pids_df = s2_pids_df[s2_pids_df['url'].str.contains('https://www.reddit.com/r/euphoria/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat comments and posts that are not None \n",
    "posts = s2_pids_df['selftext']\n",
    "# filter out the None values\n",
    "posts2 = pd.DataFrame([post for post in posts if post not in [None, '', 'None', '[deleted]', '[removed]']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "GET COMMENTS FROM EACH POST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the id of a post, get the comments and store in a dataframe\n",
    "\n",
    "def get_comments(submission_id):\n",
    "    \"\"\"\n",
    "    Get comments from a submission\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    submission = reddit.submission(id = submission_id)\n",
    "    submission.comments.replace_more(limit = None)\n",
    "    for comment in submission.comments.list():\n",
    "        comments.append({'body': comment.body, 'author': comment.author, 'score': comment.score, 'created_utc': comment.created_utc, 'id': comment.id})\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 'snb18q'\n",
    "test = get_comments(test_id)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all the posts and get all the comments\n",
    "\n",
    "s2_comments = []\n",
    "for post_id in s2_pids.id:\n",
    "    s2_comments.append(get_comments(post_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to handle the topic fading - several several layers of nested comments (?)  \n",
    "different experiements:  \n",
    "- entire set\n",
    "- 75% of comments\n",
    "- 50% of comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "GET `BODY` OF EACH COMMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(df, percent = 1):\n",
    "    body_ls = []\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(df.iloc[i])):\n",
    "            if type(df.iloc[i][j]) != type(None):\n",
    "               body_ls.append(df.iloc[i][j]['body'])\n",
    "    # adjust for percentage\n",
    "    body_ls = body_ls[:math.floor(len(body_ls) * percent)]\n",
    "    return body_ls   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# test_comm = get_body(comment_body_df_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1: entire set with all comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "100% of the comment depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply func to all the posts\n",
    "s2_comment_body_all_df = pd.DataFrame(get_body(s2_comments_df, percent = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_comment_body_all_df = pd.concat([s2_comment_body_all_df, posts2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "s2_comment_body_all_df.to_pickle('../dat/s2_rue_comments.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
