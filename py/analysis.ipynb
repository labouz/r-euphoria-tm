{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering impact of the Series 'Euphoria' through NLP\n",
    "### Analysis based on posts and comments on the `r/euphoria` subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis plan:  \n",
    "\n",
    "0. identify question(s) and data sources\n",
    "1. clean data to get it into a standard format for further analysis\n",
    "   1. corpus (collection of texts) - to dataframe using `pandas`\n",
    "   2. document-term matrix - clean, tokenize, tdm\n",
    "2. EDA\n",
    "3. topic modeling based on comments of drug-related posts on r/euphoria\n",
    "4. network analysis between users of drug and euphoria communities\n",
    "5. aspect based sentiment analysis of euphoria drug-related comments\n",
    "6. topic modeling based on comments that refer to euphoria on r/opioids, r/cannabis, r/benzodiazepenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. What is our question?\n",
    "\n",
    "*What about the drug portrayal on HBO's Euphoria makes it engaging to fans?*\n",
    "\n",
    "I: the question, \"what makes Euphoria engaged in conversations around drugs?\"  \n",
    "O: data that cleaned, organized, in a standard format that can be used in future analysis  \n",
    "\n",
    "**Data Source(s)**  \n",
    "1. Reddit - `r/euphoria`\n",
    "   1. comments\n",
    "   2. building keyword list for filetering comments  \n",
    "\n",
    "**Limit Scope**\n",
    "- Using library `praw`, pull 'top' and 'hot' headlines from subreddit\n",
    "- Filter headlines based on if any keyword appeared in the headline\n",
    "  - keyword list was created from a brief manual examination of headlines for repeated mentions of certain substances\n",
    "- Filter drug-related headlines based on the number of comments and overall relevance \n",
    "  - number of comments provided indication of how popular the headline was\n",
    "\n",
    "**Data Gathering**\n",
    "- `praw`\n",
    "  - wrapper around reddit api\n",
    "- `pickle`\n",
    "  - saving data for later\n",
    "- `pandas`\n",
    "  - exporting data to csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "comments = pd.read_csv('../dat/all_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where body = '[deleted]'\n",
    "comments = comments[comments['body'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group comments based on post id and return dictionary of post id and comments\n",
    "\n",
    "def combine_text(comments):\n",
    "    comments = comments.groupby('post').agg({'body': lambda x: ' '.join(x)})\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_combined = combine_text(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_combined['body'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Clean data  \n",
    "\n",
    "Common steps:\n",
    "- remove punctuation  \n",
    "- lowercase letters  \n",
    "- remove numbers\n",
    "\n",
    "Future steps after tokenization:  \n",
    "- stem\n",
    "- lemmatize\n",
    "- combine phrases like 'thank you' to bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from string import punctuation\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_round1(text):\n",
    "    text = text.lower()\n",
    "    # text in squre brackets\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    # punctuation\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), '', text)\n",
    "    # remove numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(comments_combined['body'].apply(round1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round two of cleaning: remove line breaks, emojis, qutes, etc.\n",
    "def clean_round2(text):\n",
    "    text = re.sub('\\n', '', text)\n",
    "    # text = re.sub('[''\"\"...]', '', text)\n",
    "    # remove emojis\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean2 = pd.DataFrame(data_clean['body'].apply(round2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round 3 cleaning: expand contractions: i'd, you've, you're etc\n",
    "import contractions\n",
    "def clean_round3(text):\n",
    "    # expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    return text\n",
    "\n",
    "round3 = lambda x: clean_round3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean3 = pd.DataFrame(data_clean2['body'].apply(round3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another round with lemmatization and stemming\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# def clean_round4(text):\n",
    "#     # lemmatization\n",
    "#     # lemmatizer = WordNetLemmatizer()\n",
    "#     # text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "#     # stemming\n",
    "#     stemmer = PorterStemmer()\n",
    "#     text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "#     return text\n",
    "\n",
    "# round4 = lambda x: clean_round4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clean4 = pd.DataFrame(data_clean3['body'].apply(round4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Organize data**  \n",
    "\n",
    "1. Corpus = `data_clean3`\n",
    "2. TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "# add column with meaningfull post identifier\n",
    "\n",
    "titles = ['Likely to try drugs', 'Elliots response to free drugs', 'Realistic portrayal of withdrawal']\n",
    "data_clean3['post_q'] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle it\n",
    "data_clean3.to_pickle('../dat/corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aana</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abby</th>\n",
       "      <th>abhorrence</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zendaya</th>\n",
       "      <th>zendayas</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 5135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aana  ab  aback  abby  abhorrence  ability  able  abroad  absolute  \\\n",
       "0   1     0   0      0     1           0        0     9       0         4   \n",
       "1   2     1   1      1     0           0        2     6       0         0   \n",
       "2   3     0   1      0     0           1        0    14       1         4   \n",
       "\n",
       "   ...  zealand  zendaya  zendayas  zero  zoloft  zombie  zone  zoo  zooming  \\\n",
       "0  ...        1        8         1     0       0       0     1    0        1   \n",
       "1  ...        0        2         0     1       0       1     0    0        0   \n",
       "2  ...        0        4         0     3       1       1     1    1        0   \n",
       "\n",
       "   zs  \n",
       "0   1  \n",
       "1   0  \n",
       "2   0  \n",
       "\n",
       "[3 rows x 5135 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean3['body'])\n",
    "data_tdm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_tdm_index = data_tdm.index\n",
    "data_tdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meh, not sure how i feel about stemming and lemmatizing.\n",
    "creates weird words that i think are more noisy than helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle for later use\n",
    "data_tdm.to_pickle('../dat/tdm.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
