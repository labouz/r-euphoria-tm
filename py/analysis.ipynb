{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering impact of the Series 'Euphoria' through NLP\n",
    "### Analysis based on posts and comments on the `r/euphoria` subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis plan:  \n",
    "\n",
    "0. identify question(s) and data sources\n",
    "1. clean data to get it into a standard format for further analysis\n",
    "   1. corpus (collection of texts) - to dataframe using `pandas`\n",
    "   2. document-term matrix - clean, tokenize, tdm\n",
    "2. EDA\n",
    "3. topic modeling based on comments referencing 'rue' posts on r/euphoria\n",
    "4. *network analysis between users of drug and euphoria communities* - time permitting\n",
    "5. aspect based sentiment analysis of euphoria drug-related comments\n",
    "6. *topic modeling based on comments that refer to euphoria on r/opioids, r/cannabis, r/benzodiazepenes* - time permitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. What is our question?\n",
    "\n",
    "*What about the drug portrayal on HBO's Euphoria makes it engaging to fans?*\n",
    "*Specifically, what topics emerge in the commments and is there a observable difference between seasons 1 and 2?*\n",
    "\n",
    "I: the question, \"what makes viewers of HBO's Euphoria engaged in online discourse on Reddit - Seasons 1 and 2?\"  \n",
    "O: data that cleaned, organized, in a standard format that can be used in future analysis  \n",
    "\n",
    "**Data Source(s)**  \n",
    "1. Reddit - `r/euphoria`\n",
    "   1. posts/comments\n",
    "   2. filtering comments on date and 'Rue'\n",
    "      1. S1:\n",
    "      2. S2: jan 9 - feb 7, 2022\n",
    "\n",
    "**Limit Scope**\n",
    "- Using library `psaw`, pull posts that mention 'Rue' during the time frame\n",
    "- Using post ids, pull all comment trees for each post\n",
    "- Experiment with tree depth : 100%, 85%, 50% or top N\n",
    "\n",
    "**Data Gathering**\n",
    "- `psaw`\n",
    "  - wrapper around pushift.io reddit api\n",
    "- `pickle`\n",
    "  - saving data for later\n",
    "- `pandas`\n",
    "  - exporting data to csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# run clean_funs.py to get the functions\n",
    "exec(open('clean_funs.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from pandas import read_pickle\n",
    "\n",
    "raw = read_pickle('../dat/s2_rue_comments.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the cell is of type 'float64', return the index\n",
    "float_indices = raw[0].apply(lambda x: x.index if type(x) == 'float64'\n",
    "                             else None).dropna().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows with float indices\n",
    "float_indices = []\n",
    "for i, value in enumerate(raw[0]):\n",
    "    if isinstance(value, float):\n",
    "        float_indices.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows with float indices\n",
    "raw_no_float = raw.drop(float_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove spammy comments\n",
    "raw_no_float['isSpam'] = raw_no_float[0].apply(clean_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows with spammy comments\n",
    "raw_no_float_no_spam = raw_no_float[raw_no_float['isSpam'] != 'spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rd/p010xzd55nl_33_67s8769q00000gq/T/ipykernel_15237/3405667418.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_no_float_no_spam['lang'] = raw_no_float_no_spam[0].apply(detect_language)\n"
     ]
    }
   ],
   "source": [
    "# remove non english comments\n",
    "\n",
    "raw_no_float_no_spam['lang'] = raw_no_float_no_spam[0].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for english\n",
    "comments_ls = raw_no_float_no_spam[raw_no_float_no_spam['lang'] == 'en'][0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Clean data  \n",
    "\n",
    "Common steps:\n",
    "- remove punctuation  \n",
    "- lowercase letters  \n",
    "- remove numbers\n",
    "\n",
    "Future steps after tokenization:  \n",
    "- stem\n",
    "- lemmatize\n",
    "- combine phrases like 'thank you' to bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = []\n",
    "for comment in comments_ls:\n",
    "    data_clean.append(round1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean2 = []\n",
    "for comment in data_clean:\n",
    "    data_clean2.append(round2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean3 = []\n",
    "for comment in data_clean2:\n",
    "    data_clean3.append(round3(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*because of the cleanning of repeated characters affecting words with double letters (addict, struggle), do stemming and lemmatizing first, then clean repeated chars*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean3a = []\n",
    "for comment in data_clean3:\n",
    "    data_clean3a.append(round3a(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify('switching to needle is baaaaaddddddd she manipulated her into needle use planning to pimp her out to pay her debt dark af')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean4 = []\n",
    "for comment in data_clean3a:\n",
    "    data_clean4.append(round4(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are a lot of words with similar roots: play, played, playing\n",
    "\n",
    "also, somehow a lot of random posts ended up in the clean set:  \n",
    "- ' shiping usa only payment paypal fampf buyer pay fe for paypal gampsdecants ml or ml htpsimgurcomalhnzx htpsimgurcomaiozvyq of botles and respective level measured with a syringe for reference ptfe tape betwen botle and nozle thread and parafilm around where nozle and botle met decants are individualy buble wraped before going inside zip bag and then inside buble mailersthanks for loking please chatpm if interested or if you have any questionsampxb htpsimgurcomalhnzx link for a photo of botlesmlmlgardenia antiguaorangerie venisepivoine suzhourose darabierogue malachitesable nuitvert malachiteampxb htpsimgurcomasvqrut link for a photo of botlesmlml rue cambonbeigebois de ilescoromandelcristale edtcristale eau vertela pausale liono edtno edpno poudreno sycomoreampxb htpsimgurcomausue link for a photo of botlesmlmlambre nuitbois dargentcuir canagebalade sauvagebele de jourdioramoureau noirefeve delicieusegrand balgris diorholy peonyjasmin de angesla cole noiremilylaforetmitzahnew lok oud ispahanoud rosewodpatchouli imperialpurple oudsantal noirvanila dioramavetiverampxb htpsimgurcomagoari link for a photo of botlesmlmlvelvet amber sunvelvet desert oudvelvet exotic leathervelvet incensovelvet mimosa blomvelvet tender oudampxb htpsimgurcomaksojbo link for a photo of botlesmlmlmlcarnal flowerlys mediteranethe monthe nightampxb htpsimgurcomazdbkwra link for a photo of botlesmlmla chant for the nympha song for the rosethe eye of the tigerthe voice of the snakeampxb htpsimgurcomahipx link for a photo of botlesmlmlangelique noirebois darmeniecuir belugaencens mythiquejoyeuse tubereuseneroli outrenoirsantal royalshalimar milsime vanila planifoliampxb htpsimgurcomalibgsh link for a photo of botlesmlmlamyris homeaqua universalis fortegrand soiroud extraitoud satin modampxb htpsimgurcomaeysuwsh link for a photo of botlesmlmlbabylondark lightday for nightdesert serenademarienbadmiracle of the roseampxb htpsimgurcomagdqne link for a photo of botlesmlmlambre sultanborneo cuir mauresquede profundisfumerie turquetubereuse crimineleampxb htpsimgurcomajxlzlf link for a photo of botlesmlml rue de belechase saint place sulpiceatlas gardencabancaftancapelinexquisite embroiderymagnificent goldsaharienesplendid wodtrenchtuxedoveloursvinyle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Organize data**  \n",
    "\n",
    "1. Corpus = `data_clean4`\n",
    "2. TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean4_df = pd.DataFrame(data_clean4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any empty rows\n",
    "data_clean5_df = data_clean4_df[data_clean4_df[0] != '']\n",
    "data_clean5_df = data_clean5_df[data_clean5_df[0] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/_libs/index.pyx:144\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index_class_helper.pxi:41\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: True",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/fes/py/analysis.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/analysis.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# minor cleaning\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/analysis.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# remove rows that only have 1 word\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/analysis.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data_clean5_df2 \u001b[39m=\u001b[39m data_clean5_df[\u001b[39m0\u001b[39;49m][\u001b[39mlen\u001b[39;49m(data_clean5_df[\u001b[39m0\u001b[39;49m]) \u001b[39m>\u001b[39;49m \u001b[39m1\u001b[39;49m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    957\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    960\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    961\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1068\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1070\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: True"
     ]
    }
   ],
   "source": [
    "# minor cleaning\n",
    "# keep rows with length > 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [18344,18631,18743,18997, 19139, 18272 , 18340,\n",
    "    18991\t,\n",
    "18062\t,\n",
    "18063\t,\n",
    "18063\t,\n",
    "18271\t,\n",
    "18271\t,\n",
    "18300\t,\n",
    "18300\t,\n",
    "18337\t,\n",
    "18337\t,\n",
    "18474\t,\n",
    "18474\t,\n",
    "18621\t,\n",
    "18621\t,\n",
    "18622\t,\n",
    "18622\t,\n",
    "18629\t,\n",
    "18629\t,\n",
    "18639\t,\n",
    "18639\t,\n",
    "18666\t,\n",
    "18666\t,\n",
    "18667\t,\n",
    "18667\t,\n",
    "18668\t,\n",
    "18668\t,\n",
    "18686\t,\n",
    "18686\t,\n",
    "18690\t,\n",
    "18690\t,\n",
    "18695\t,\n",
    "18695\t,\n",
    "18697\t,\n",
    "18697\t,\n",
    "18742\t,\n",
    "18742\t,\n",
    "18753\t,\n",
    "18753\t,\n",
    "18845\t,\n",
    "18846\t,\n",
    "18846\t,\n",
    "18848\t,\n",
    "18848\t,\n",
    "18851\t,\n",
    "18870\t,\n",
    "18870\t,\n",
    "18871\t,\n",
    "18871\t,\n",
    "18872\t,\n",
    "18872\t,\n",
    "18873\t,\n",
    "18873\t,\n",
    "18874\t,\n",
    "18874\t,\n",
    "18875\t,\n",
    "18875\t,\n",
    "18876\t,\n",
    "18876\t,\n",
    "18880\t,\n",
    "18880\t,\n",
    "18881\t,\n",
    "18881\t,\n",
    "18882\t,\n",
    "18882\t,\n",
    "18884\t,\n",
    "18884\t,\n",
    "18885\t,\n",
    "18885\t,\n",
    "18886\t,\n",
    "18886\t,\n",
    "18887\t,\n",
    "18887\t,\n",
    "18888\t,\n",
    "18888\t,\n",
    "18890\t,\n",
    "18890\t,\n",
    "18891\t,\n",
    "18891\t,\n",
    "18895\t,\n",
    "18895\t,\n",
    "18896\t,\n",
    "18896\t,\n",
    "18897\t,\n",
    "18897\t,\n",
    "18899\t,\n",
    "18899\t,\n",
    "18900\t,\n",
    "18900\t,\n",
    "18902\t,\n",
    "18902\t,\n",
    "18903\t,\n",
    "18903\t,\n",
    "18904\t,\n",
    "18904\t,\n",
    "18931\t,\n",
    "18931\t,\n",
    "18949\t,\n",
    "18949\t,\n",
    "18950\t,\n",
    "18950\t,\n",
    "18951\t,\n",
    "18951\t,\n",
    "18952\t,\n",
    "18952\t,\n",
    "18953\t,\n",
    "18953\t,\n",
    "18954\t,\n",
    "18954\t,\n",
    "18955\t,\n",
    "18955\t,\n",
    "18956\t,\n",
    "18956\t,\n",
    "18957\t,\n",
    "18957\t,\n",
    "18958\t,\n",
    "18958\t,\n",
    "18962\t,\n",
    "18962\t,\n",
    "18980\t,\n",
    "18980\t,\n",
    "18981\t,\n",
    "18981\t,\n",
    "19003\t,\n",
    "19003\t,\n",
    "19009\t,\n",
    "19009\t,\n",
    "19015\t,\n",
    "19015\t,\n",
    "19016\t,\n",
    "19016\t,\n",
    "19018\t,\n",
    "19018\t,\n",
    "19020\t,\n",
    "19020\t,\n",
    "19021\t,\n",
    "19021\t,\n",
    "19023\t,\n",
    "19023\t,\n",
    "19024\t,\n",
    "19024\t,\n",
    "19027\t,\n",
    "19027\t,\n",
    "19028\t,\n",
    "19028\t,\n",
    "19029\t,\n",
    "19029\t,\n",
    "19093\t,\n",
    "19093\t,\n",
    "19094\t,\n",
    "19094\t,\n",
    "19095\t,\n",
    "19095\t,\n",
    "19096\t,\n",
    "19096\t,\n",
    "19098\t,\n",
    "19098\t,\n",
    "19099\t,\n",
    "19099\t,\n",
    "19100\t,\n",
    "19100\t,\n",
    "19101\t,\n",
    "19101\t,\n",
    "19102\t,\n",
    "19102\t,\n",
    "19104\t,\n",
    "19104\t,\n",
    "19146\t,\n",
    "19146\t,\n",
    "19249\t,\n",
    "19249\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct\n",
    "exclude = list(set(exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove excluded indices from data_clean4_df\n",
    "data_clean5_df = data_clean4_df.drop(data_clean4_df.index[exclude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle it\n",
    "data_clean5_df.to_pickle('../dat/corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abashed</th>\n",
       "      <th>abducts</th>\n",
       "      <th>abe</th>\n",
       "      <th>abey</th>\n",
       "      <th>abhorent</th>\n",
       "      <th>...</th>\n",
       "      <th>zongao</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zora</th>\n",
       "      <th>zorbcaps</th>\n",
       "      <th>zoted</th>\n",
       "      <th>zouabi</th>\n",
       "      <th>zoya</th>\n",
       "      <th>zrue</th>\n",
       "      <th>zsuzsana</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19169</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19172</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19173</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19174 rows × 23966 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aback  abandon  abandoned  abandoning  abandonment  abashed  abducts  \\\n",
       "0          0        0          0           0            0        0        0   \n",
       "1          0        0          0           0            0        0        0   \n",
       "2          0        0          0           0            0        0        0   \n",
       "3          0        0          0           0            0        0        0   \n",
       "4          0        0          0           0            0        0        0   \n",
       "...      ...      ...        ...         ...          ...      ...      ...   \n",
       "19169      0        0          0           0            0        0        0   \n",
       "19170      0        0          0           0            0        0        0   \n",
       "19171      0        0          0           0            0        0        0   \n",
       "19172      0        0          0           0            0        0        0   \n",
       "19173      0        0          0           0            0        0        0   \n",
       "\n",
       "       abe  abey  abhorent  ...  zongao  zoning  zora  zorbcaps  zoted  \\\n",
       "0        0     0         0  ...       0       0     0         0      0   \n",
       "1        0     0         0  ...       0       0     0         0      0   \n",
       "2        0     0         0  ...       0       0     0         0      0   \n",
       "3        0     0         0  ...       0       0     0         0      0   \n",
       "4        0     0         0  ...       0       0     0         0      0   \n",
       "...    ...   ...       ...  ...     ...     ...   ...       ...    ...   \n",
       "19169    0     0         0  ...       0       0     0         0      0   \n",
       "19170    0     0         0  ...       0       0     0         0      0   \n",
       "19171    0     0         0  ...       0       0     0         0      0   \n",
       "19172    0     0         0  ...       0       0     0         0      0   \n",
       "19173    0     0         0  ...       0       0     0         0      0   \n",
       "\n",
       "       zouabi  zoya  zrue  zsuzsana  zurich  \n",
       "0           0     0     0         0       0  \n",
       "1           0     0     0         0       0  \n",
       "2           0     0     0         0       0  \n",
       "3           0     0     0         0       0  \n",
       "4           0     0     0         0       0  \n",
       "...       ...   ...   ...       ...     ...  \n",
       "19169       0     0     0         0       0  \n",
       "19170       0     0     0         0       0  \n",
       "19171       0     0     0         0       0  \n",
       "19172       0     0     0         0       0  \n",
       "19173       0     0     0         0       0  \n",
       "\n",
       "[19174 rows x 23966 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "add_stop_words = ['i', 'just','did', 'ab', 'amp', 'ml', 'xb','abc', 'abcb', 'abcny', 'abd', 'abdabca', 'fs', \n",
    "                  'zpqxhxhzanapjsjbf', 'zqcsrpwsge', 'zqnuhckwdqwrhkuo', 'zs', 'zshwbhethehenozxfyqg',\n",
    "                  'zsmkbrmwngzsibrntkt', 'zy', 'zwhnrmujykdxmntiub', 'afqjcnguytghbsuvixmglpwzqbg', 'ebecadcbdfcbafbdb',\n",
    "                  'abfbmltmqspf', 'abfafebfbad', 'abaedefabdfef', 'abafbfbedbada', 'her', 'him',  'and']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_clean5_df[0])\n",
    "data_tdm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_tdm_index = data_tdm.index\n",
    "data_tdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meh, not sure how i feel about stemming and lemmatizing.\n",
    "creates weird words that i think are more noisy than helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle for later use\n",
    "data_tdm.to_pickle('../dat/tdm.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
