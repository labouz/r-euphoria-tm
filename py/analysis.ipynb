{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering impact of the Series 'Euphoria' through NLP\n",
    "### Analysis based on posts and comments on the `r/euphoria` subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis plan:  \n",
    "\n",
    "0. identify question(s) and data sources\n",
    "1. clean data to get it into a standard format for further analysis\n",
    "   1. corpus (collection of texts) - to dataframe using `pandas`\n",
    "   2. document-term matrix - clean, tokenize, tdm\n",
    "2. EDA\n",
    "3. topic modeling based on comments referencing 'rue' posts on r/euphoria\n",
    "4. *network analysis between users of drug and euphoria communities* - time permitting\n",
    "5. aspect based sentiment analysis of euphoria drug-related comments\n",
    "6. *topic modeling based on comments that refer to euphoria on r/opioids, r/cannabis, r/benzodiazepenes* - time permitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. What is our question?\n",
    "\n",
    "*What about the drug portrayal on HBO's Euphoria makes it engaging to fans?*\n",
    "*Specifically, what topics emerge in the commments and is there a observable difference between seasons 1 and 2?*\n",
    "\n",
    "I: the question, \"what makes viewers of HBO's Euphoria engaged in online discourse on Reddit - Seasons 1 and 2?\"  \n",
    "O: data that cleaned, organized, in a standard format that can be used in future analysis  \n",
    "\n",
    "**Data Source(s)**  \n",
    "1. Reddit - `r/euphoria`\n",
    "   1. posts/comments\n",
    "   2. filtering comments on date and 'Rue'\n",
    "      1. S1:\n",
    "      2. S2: jan 9 - feb 7, 2022\n",
    "\n",
    "**Limit Scope**\n",
    "- Using library `psaw`, pull posts that mention 'Rue' during the time frame\n",
    "- Using post ids, pull all comment trees for each post\n",
    "- Experiment with tree depth : 100%, 85%, 50% or top N\n",
    "\n",
    "**Data Gathering**\n",
    "- `psaw`\n",
    "  - wrapper around pushift.io reddit api\n",
    "- `pickle`\n",
    "  - saving data for later\n",
    "- `pandas`\n",
    "  - exporting data to csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from pandas import read_pickle\n",
    "\n",
    "raw = read_pickle('../dat/s2_rue_comments.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments = raw[raw[0] != '[deleted]']\n",
    "# remove any row that contatins an automatically generated response by the bot\n",
    "pattern_del = 'Thank you for your submission'\n",
    "filtered = comments[0].str.contains(pattern_del, na = False)\n",
    "comments = comments[~filtered]\n",
    "# remove solicitation comments\n",
    "pattern_ads = 'Paypal'\n",
    "filtered_ads = comments[0].str.contains(pattern_ads, na = False)\n",
    "comments = comments[~filtered_ads]\n",
    "# more solicitations\n",
    "pattern3 = 'Here is [Rue 21 Coupon Code]'\n",
    "comments = comments[~comments[0].str.contains(pattern3, na = False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern4 = '*PAYPAL INVOICE ONLY*'\n",
    "# comments = comments[~comments[0].str.contains(pattern4, na = False)]\n",
    "# \n",
    "pattern5 = 'JAX\\_Jacksonville\\_127'\n",
    "comments = comments[~comments[0].str.contains(pattern5, na = False)]\n",
    "# some commentary on edgar allen poe?\n",
    "pattern7 = 'The word detective did not'\n",
    "comments = comments[~comments[0].str.contains(pattern7, na = False)]\n",
    "# \n",
    "pattern8 = 'On my way to trinoma/vertis north'\n",
    "comments = comments[~comments[0].str.contains(pattern8, na = False)]\n",
    "# \n",
    "pattern10 = 'Shipping:'\n",
    "comments = comments[~comments[0].str.contains(pattern10, na = False)] \n",
    "# promoting music\n",
    "pattern11 = 'Albums'\n",
    "comments = comments[~comments[0].str.contains(pattern11, na = False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non=english comments\n",
    "from langdetect import detect\n",
    "\n",
    "# add a new column for language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "comments['lang'] = comments[0].apply(detect_language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for english\n",
    "comments = comments[comments['lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_ls = comments[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Clean data  \n",
    "\n",
    "Common steps:\n",
    "- remove punctuation  \n",
    "- lowercase letters  \n",
    "- remove numbers\n",
    "\n",
    "Future steps after tokenization:  \n",
    "- stem\n",
    "- lemmatize\n",
    "- combine phrases like 'thank you' to bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from string import punctuation\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and remove numbers\n",
    "# def cleaning_numbers(data):\n",
    "#     return re.sub('[0-9]+', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regex - clean and remove URLs\n",
    "def cleaning_URLs(text):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_round1(text):\n",
    "    # convert all to string\n",
    "    text = str(text)\n",
    "    # lower\n",
    "    text = text.lower()\n",
    "    # text in squre brackets\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    # urls\n",
    "    text = cleaning_URLs(text)\n",
    "    # punctuation\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), ' ', text)\n",
    "    # remove numbers\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    # it looks like there are edits being made to comments\n",
    "    # remove any instance of edit:\n",
    "    text = re.sub('edit:', '', text)\n",
    "    # remove any user handles\n",
    "    text = re.sub('@[a-zA-Z0-9_]+', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = []\n",
    "for comment in comments_ls:\n",
    "    data_clean.append(round1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round two of cleaning: remove line breaks, emojis, qutes, etc.\n",
    "def clean_round2(text):\n",
    "    text = re.sub('\\n', '', text)\n",
    "    # text = re.sub('[''\"\"...]', '', text)\n",
    "    # remove emojis\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean2 = []\n",
    "for comment in data_clean:\n",
    "    data_clean2.append(round2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round 3 cleaning: expand contractions: i'd, you've, you're etc\n",
    "import contractions\n",
    "def clean_round3(text):\n",
    "    # expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    return text\n",
    "\n",
    "round3 = lambda x: clean_round3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean3 = []\n",
    "for comment in data_clean2:\n",
    "    data_clean3.append(round3(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*because of the cleanning of repeated characters affecting words with double letters (addict, struggle), do stemming and lemmatizing first, then clean repeated chars*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another round with lemmatization and stemming\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def clean_round3a(text):\n",
    "    # stemming\n",
    "    # stemmer = PorterStemmer()\n",
    "    # text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "round3a = lambda x: clean_round3a(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean3a = []\n",
    "for comment in data_clean3:\n",
    "    data_clean3a.append(round3a(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the 500 most common words in all the comments to use to populate a\n",
    "# list of words to be excluded in the modify function\n",
    "# def get_top_n_words(corpus, n=None):\n",
    "#     vectorizer = CountVectorizer(max_features=n, stop_words='english')\n",
    "#     vectorizer.fit_transform(corpus)\n",
    "#     bag_of_words = vectorizer.vocabulary_\n",
    "#     # sort the words by their frequency\n",
    "#     bag_of_words = sorted(bag_of_words.items(), key=lambda x: x[1], reverse=True)\n",
    "#     return bag_of_words\n",
    "\n",
    "# top = get_top_n_words(data_clean3a, n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove accented characters\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "# clean and remove repeated characters\n",
    "# def cleaning_repeating_char(text):\n",
    "#     return re.sub(r'(.)1+', r'1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_not_mod_ls = ['good', 'spelling', 'telling', 'addiction','addicts', 'finally', 'all', 'personally', 'struggles', 'really', 'cassie', 'free', \n",
    "                 'feeling', 'elliot', 'maddy', 'literally', 'getting', 'better', 'actually', 'totally', 'telling', 'supposed', 'teen',\n",
    "                 'stuff', 'sorry', 'soon', 'sells', 'sees', 'schools', 'rooms', 'redeeming', 'reddit', 'putting', 'pretty', 'official', 'need',\n",
    "                 'messages', 'matters', 'looks', 'little', 'kills', 'issues', 'horror', 'hell', 'happens', 'happy', 'gonna', 'getting', 'especially',\n",
    "                 'different', 'classic', 'businesses', 'attention', 'basically', 'apples', 'weeks', 'streets', 'needles', 'planning', 'full', 'fully',\n",
    "                 'agreed', 'will', 'been', 'seeds', 'desserts', 'google', 'seen', 'addictions', 'foot', 'funny', 'comments', 'comment', 'poll',\n",
    "                 'channel', 'well', 'wall', 'hall']\n",
    "\n",
    "from operator import contains\n",
    "def modify(s):\n",
    "    # split comments into words\n",
    "    comment = []\n",
    "    for word in s.split():\n",
    "        if any(word in x for x in do_not_mod_ls if contains(x, word)):\n",
    "            comment.append(word)\n",
    "        else:\n",
    "            word = re.sub(r'([a-z])\\1+', r'\\1', word)\n",
    "            comment.append(word)\n",
    "    # join the words back together\n",
    "    comment = ' '.join(comment)\n",
    "    return comment\n",
    "    \n",
    "# print(modify('good'))\n",
    "# print(modify('waaayyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'switching to needle is bad she manipulated her into needle use planning to pimp her out to pay her debt dark af'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify('switching to needle is baaaaaddddddd she manipulated her into needle use planning to pimp her out to pay her debt dark af')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove repeating characters and non unicode characters\n",
    "def clean_round4(text):\n",
    "    # text = remove_accented_chars(text)\n",
    "    # text = cleaning_repeating_char(text)\n",
    "    # remove non unicode characters\n",
    "    text = re.sub('[^\\x00-\\x7F]+', '', text)\n",
    "    # remove repeating characters\n",
    "    text = modify(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "round4 = lambda x: clean_round4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean4 = []\n",
    "for comment in data_clean3a:\n",
    "    data_clean4.append(round4(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are a lot of words with similar roots: play, played, playing\n",
    "\n",
    "also, somehow a lot of random posts ended up in the clean set:  \n",
    "- ' shiping usa only payment paypal fampf buyer pay fe for paypal gampsdecants ml or ml htpsimgurcomalhnzx htpsimgurcomaiozvyq of botles and respective level measured with a syringe for reference ptfe tape betwen botle and nozle thread and parafilm around where nozle and botle met decants are individualy buble wraped before going inside zip bag and then inside buble mailersthanks for loking please chatpm if interested or if you have any questionsampxb htpsimgurcomalhnzx link for a photo of botlesmlmlgardenia antiguaorangerie venisepivoine suzhourose darabierogue malachitesable nuitvert malachiteampxb htpsimgurcomasvqrut link for a photo of botlesmlml rue cambonbeigebois de ilescoromandelcristale edtcristale eau vertela pausale liono edtno edpno poudreno sycomoreampxb htpsimgurcomausue link for a photo of botlesmlmlambre nuitbois dargentcuir canagebalade sauvagebele de jourdioramoureau noirefeve delicieusegrand balgris diorholy peonyjasmin de angesla cole noiremilylaforetmitzahnew lok oud ispahanoud rosewodpatchouli imperialpurple oudsantal noirvanila dioramavetiverampxb htpsimgurcomagoari link for a photo of botlesmlmlvelvet amber sunvelvet desert oudvelvet exotic leathervelvet incensovelvet mimosa blomvelvet tender oudampxb htpsimgurcomaksojbo link for a photo of botlesmlmlmlcarnal flowerlys mediteranethe monthe nightampxb htpsimgurcomazdbkwra link for a photo of botlesmlmla chant for the nympha song for the rosethe eye of the tigerthe voice of the snakeampxb htpsimgurcomahipx link for a photo of botlesmlmlangelique noirebois darmeniecuir belugaencens mythiquejoyeuse tubereuseneroli outrenoirsantal royalshalimar milsime vanila planifoliampxb htpsimgurcomalibgsh link for a photo of botlesmlmlamyris homeaqua universalis fortegrand soiroud extraitoud satin modampxb htpsimgurcomaeysuwsh link for a photo of botlesmlmlbabylondark lightday for nightdesert serenademarienbadmiracle of the roseampxb htpsimgurcomagdqne link for a photo of botlesmlmlambre sultanborneo cuir mauresquede profundisfumerie turquetubereuse crimineleampxb htpsimgurcomajxlzlf link for a photo of botlesmlml rue de belechase saint place sulpiceatlas gardencabancaftancapelinexquisite embroiderymagnificent goldsaharienesplendid wodtrenchtuxedoveloursvinyle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Organize data**  \n",
    "\n",
    "1. Corpus = `data_clean4`\n",
    "2. TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean4_df = pd.DataFrame(data_clean4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\n",
    "    18991\t,\n",
    "18062\t,\n",
    "18063\t,\n",
    "18063\t,\n",
    "18271\t,\n",
    "18271\t,\n",
    "18300\t,\n",
    "18300\t,\n",
    "18337\t,\n",
    "18337\t,\n",
    "18474\t,\n",
    "18474\t,\n",
    "18621\t,\n",
    "18621\t,\n",
    "18622\t,\n",
    "18622\t,\n",
    "18629\t,\n",
    "18629\t,\n",
    "18639\t,\n",
    "18639\t,\n",
    "18666\t,\n",
    "18666\t,\n",
    "18667\t,\n",
    "18667\t,\n",
    "18668\t,\n",
    "18668\t,\n",
    "18686\t,\n",
    "18686\t,\n",
    "18690\t,\n",
    "18690\t,\n",
    "18695\t,\n",
    "18695\t,\n",
    "18697\t,\n",
    "18697\t,\n",
    "18742\t,\n",
    "18742\t,\n",
    "18753\t,\n",
    "18753\t,\n",
    "18845\t,\n",
    "18846\t,\n",
    "18846\t,\n",
    "18848\t,\n",
    "18848\t,\n",
    "18851\t,\n",
    "18870\t,\n",
    "18870\t,\n",
    "18871\t,\n",
    "18871\t,\n",
    "18872\t,\n",
    "18872\t,\n",
    "18873\t,\n",
    "18873\t,\n",
    "18874\t,\n",
    "18874\t,\n",
    "18875\t,\n",
    "18875\t,\n",
    "18876\t,\n",
    "18876\t,\n",
    "18880\t,\n",
    "18880\t,\n",
    "18881\t,\n",
    "18881\t,\n",
    "18882\t,\n",
    "18882\t,\n",
    "18884\t,\n",
    "18884\t,\n",
    "18885\t,\n",
    "18885\t,\n",
    "18886\t,\n",
    "18886\t,\n",
    "18887\t,\n",
    "18887\t,\n",
    "18888\t,\n",
    "18888\t,\n",
    "18890\t,\n",
    "18890\t,\n",
    "18891\t,\n",
    "18891\t,\n",
    "18895\t,\n",
    "18895\t,\n",
    "18896\t,\n",
    "18896\t,\n",
    "18897\t,\n",
    "18897\t,\n",
    "18899\t,\n",
    "18899\t,\n",
    "18900\t,\n",
    "18900\t,\n",
    "18902\t,\n",
    "18902\t,\n",
    "18903\t,\n",
    "18903\t,\n",
    "18904\t,\n",
    "18904\t,\n",
    "18931\t,\n",
    "18931\t,\n",
    "18949\t,\n",
    "18949\t,\n",
    "18950\t,\n",
    "18950\t,\n",
    "18951\t,\n",
    "18951\t,\n",
    "18952\t,\n",
    "18952\t,\n",
    "18953\t,\n",
    "18953\t,\n",
    "18954\t,\n",
    "18954\t,\n",
    "18955\t,\n",
    "18955\t,\n",
    "18956\t,\n",
    "18956\t,\n",
    "18957\t,\n",
    "18957\t,\n",
    "18958\t,\n",
    "18958\t,\n",
    "18962\t,\n",
    "18962\t,\n",
    "18980\t,\n",
    "18980\t,\n",
    "18981\t,\n",
    "18981\t,\n",
    "19003\t,\n",
    "19003\t,\n",
    "19009\t,\n",
    "19009\t,\n",
    "19015\t,\n",
    "19015\t,\n",
    "19016\t,\n",
    "19016\t,\n",
    "19018\t,\n",
    "19018\t,\n",
    "19020\t,\n",
    "19020\t,\n",
    "19021\t,\n",
    "19021\t,\n",
    "19023\t,\n",
    "19023\t,\n",
    "19024\t,\n",
    "19024\t,\n",
    "19027\t,\n",
    "19027\t,\n",
    "19028\t,\n",
    "19028\t,\n",
    "19029\t,\n",
    "19029\t,\n",
    "19093\t,\n",
    "19093\t,\n",
    "19094\t,\n",
    "19094\t,\n",
    "19095\t,\n",
    "19095\t,\n",
    "19096\t,\n",
    "19096\t,\n",
    "19098\t,\n",
    "19098\t,\n",
    "19099\t,\n",
    "19099\t,\n",
    "19100\t,\n",
    "19100\t,\n",
    "19101\t,\n",
    "19101\t,\n",
    "19102\t,\n",
    "19102\t,\n",
    "19104\t,\n",
    "19104\t,\n",
    "19146\t,\n",
    "19146\t,\n",
    "19249\t,\n",
    "19249\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct\n",
    "exclude = list(set(exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove excluded indices from data_clean4_df\n",
    "data_clean5_df = data_clean4_df.drop(data_clean4_df.index[exclude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any empty rows\n",
    "data_clean5_df = data_clean5_df[data_clean5_df[0] != '']\n",
    "data_clean5_df = data_clean5_df[data_clean5_df[0] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle it\n",
    "data_clean5_df.to_pickle('../dat/corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.snowball import SnowballStemmer\n",
    " \n",
    "# # experiment with stemming\n",
    "# data_clean5_df_a = data_clean5_df[0].apply(lambda x: ' '.join([SnowballStemmer(language = 'english').stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaedefabdfef</th>\n",
       "      <th>abafbfbedbada</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abashed</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zora</th>\n",
       "      <th>zorbcaps</th>\n",
       "      <th>zoted</th>\n",
       "      <th>zouabi</th>\n",
       "      <th>zoya</th>\n",
       "      <th>zqcsrpwsge</th>\n",
       "      <th>zrue</th>\n",
       "      <th>zs</th>\n",
       "      <th>zsuzsana</th>\n",
       "      <th>zy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19161</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19163</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19165</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19166 rows × 23259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ab  aback  abaedefabdfef  abafbfbedbada  abandon  abandoned  \\\n",
       "0       0      0              0              0        0          0   \n",
       "1       0      0              0              0        0          0   \n",
       "2       0      0              0              0        0          0   \n",
       "3       0      0              0              0        0          0   \n",
       "4       0      0              0              0        0          0   \n",
       "...    ..    ...            ...            ...      ...        ...   \n",
       "19161   0      0              0              0        0          0   \n",
       "19162   0      0              0              0        0          0   \n",
       "19163   0      0              0              0        0          0   \n",
       "19164   0      0              0              0        0          0   \n",
       "19165   0      0              0              0        0          0   \n",
       "\n",
       "       abandoning  abandonment  abashed  abc  ...  zora  zorbcaps  zoted  \\\n",
       "0               0            0        0    0  ...     0         0      0   \n",
       "1               0            0        0    0  ...     0         0      0   \n",
       "2               0            0        0    0  ...     0         0      0   \n",
       "3               0            0        0    0  ...     0         0      0   \n",
       "4               0            0        0    0  ...     0         0      0   \n",
       "...           ...          ...      ...  ...  ...   ...       ...    ...   \n",
       "19161           0            0        0    0  ...     0         0      0   \n",
       "19162           0            0        0    0  ...     0         0      0   \n",
       "19163           0            0        0    0  ...     0         0      0   \n",
       "19164           0            0        0    0  ...     0         0      0   \n",
       "19165           0            0        0    0  ...     0         0      0   \n",
       "\n",
       "       zouabi  zoya  zqcsrpwsge  zrue  zs  zsuzsana  zy  \n",
       "0           0     0           0     0   0         0   0  \n",
       "1           0     0           0     0   0         0   0  \n",
       "2           0     0           0     0   0         0   0  \n",
       "3           0     0           0     0   0         0   0  \n",
       "4           0     0           0     0   0         0   0  \n",
       "...       ...   ...         ...   ...  ..       ...  ..  \n",
       "19161       0     0           0     0   0         0   0  \n",
       "19162       0     0           0     0   0         0   0  \n",
       "19163       0     0           0     0   0         0   0  \n",
       "19164       0     0           0     0   0         0   0  \n",
       "19165       0     0           0     0   0         0   0  \n",
       "\n",
       "[19166 rows x 23259 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean5_df[0])\n",
    "data_tdm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_tdm_index = data_tdm.index\n",
    "data_tdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meh, not sure how i feel about stemming and lemmatizing.\n",
    "creates weird words that i think are more noisy than helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle for later use\n",
    "data_tdm.to_pickle('../dat/tdm.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
