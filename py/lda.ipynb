{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering impact of the Series 'Euphoria' through NLP\n",
    "### Analysis based on posts and comments on the `r/euphoria` subreddit  \n",
    "\n",
    "#### 3. LDA - Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= *Every documents is probability dist of topics*\n",
    "\n",
    "*goal*: LDA learns the topic mix in each doc, then words in each topic   \n",
    "\n",
    "*how*: LDA randomly assigns topics to words (will be wrong). Then, iterativly, looks for how often the topic occus in the doc and how often the word occurs in the topic overall. Based on this infor, assign the word a new topic.\n",
    "\n",
    "`k = 2` is a good starting part for number of topics  \n",
    "\n",
    "*input*: TDM, K, num iterations  \n",
    "*output*: top words in each topic - figure out if they make sense\n",
    "\n",
    "*tools*:  \n",
    "`gensim`\n",
    "\n",
    "alternate factorization methods: \n",
    "- NMF\n",
    "- LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 posts in the r/euphoria subreddit  \n",
    "N ~ 1,709 comments\n",
    "1. Question: Does euphoria make you less likely to try drugs? Or are you more curious than you were before?\n",
    "2. Not enough people are talking about Elliot's response to Rue telling him about her plan to get \"free\" drugs from Laurie\n",
    "3. As an ex-opioid addict, Zendaya's withdrawal scenes are the most realistic portrayal I've ever seen before. her acting is phenomenal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aana</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abby</th>\n",
       "      <th>abhorrence</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zendaya</th>\n",
       "      <th>zendayas</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>smur2x</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sn2vpk</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqhl33</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 5121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aana  ab  aback  abby  abhorrence  ability  able  abroad  \\\n",
       "post                                                                   \n",
       "smur2x   1     0   0      0     1           0        0     9       0   \n",
       "sn2vpk   2     1   1      1     0           0        2     6       0   \n",
       "sqhl33   3     0   1      0     0           1        0    14       1   \n",
       "\n",
       "        absolute  ...  zealand  zendaya  zendayas  zero  zoloft  zombie  zone  \\\n",
       "post              ...                                                           \n",
       "smur2x         4  ...        1        8         1     0       0       0     1   \n",
       "sn2vpk         0  ...        0        2         0     1       0       1     0   \n",
       "sqhl33         4  ...        0        4         0     3       1       1     1   \n",
       "\n",
       "        zoo  zooming  zs  \n",
       "post                      \n",
       "smur2x    0        1   1  \n",
       "sn2vpk    0        0   0  \n",
       "sqhl33    1        0   0  \n",
       "\n",
       "[3 rows x 5121 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bring in data\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('../dat/tdm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>post</th>\n",
       "      <th>smur2x</th>\n",
       "      <th>sn2vpk</th>\n",
       "      <th>sqhl33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aana</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abby</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "post   smur2x  sn2vpk  sqhl33\n",
       "aa          1       2       3\n",
       "aana        0       1       0\n",
       "ab          0       1       1\n",
       "aback       0       1       0\n",
       "abby        1       0       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put tdm in gensim format\n",
    "sparse_counts = sp.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary of all terms - required by gensim\n",
    "# cv contains the whole vocabulary of the corpus\n",
    "cv = pickle.load(open('../dat/cv_Stop.pkl', 'rb'))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have corpus and id2word, now we can create the lda model\n",
    "# specify other parameters\n",
    "# more passes, more it may make sense\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"weed\" + 0.010*\"makes\" + 0.009*\"try\" + 0.007*\"opiates\" + 0.006*\"season\" + 0.006*\"watching\" + 0.006*\"euphoria\" + 0.006*\"drug\" + 0.006*\"life\" + 0.006*\"feel\"'),\n",
       " (1,\n",
       "  '0.009*\"think\" + 0.009*\"elliot\" + 0.007*\"addict\" + 0.007*\"jules\" + 0.006*\"going\" + 0.006*\"drug\" + 0.005*\"got\" + 0.004*\"said\" + 0.004*\"laurie\" + 0.004*\"good\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"try\" + 0.001*\"think\" + 0.000*\"makes\" + 0.000*\"weed\" + 0.000*\"drug\" + 0.000*\"addict\" + 0.000*\"opiates\" + 0.000*\"going\" + 0.000*\"season\" + 0.000*\"doing\"'),\n",
       " (1,\n",
       "  '0.012*\"weed\" + 0.011*\"makes\" + 0.010*\"try\" + 0.007*\"opiates\" + 0.007*\"season\" + 0.007*\"watching\" + 0.006*\"euphoria\" + 0.006*\"life\" + 0.006*\"drug\" + 0.006*\"feel\"'),\n",
       " (2,\n",
       "  '0.010*\"think\" + 0.009*\"elliot\" + 0.008*\"addict\" + 0.007*\"jules\" + 0.006*\"going\" + 0.006*\"drug\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"laurie\" + 0.004*\"good\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda for 3 topics\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"weed\" + 0.001*\"try\" + 0.001*\"makes\" + 0.000*\"think\" + 0.000*\"going\" + 0.000*\"opiates\" + 0.000*\"watching\" + 0.000*\"make\" + 0.000*\"drug\" + 0.000*\"got\"'),\n",
       " (1,\n",
       "  '0.008*\"withdrawal\" + 0.006*\"addict\" + 0.006*\"yawning\" + 0.006*\"going\" + 0.005*\"episode\" + 0.004*\"feel\" + 0.004*\"clean\" + 0.004*\"withdrawals\" + 0.004*\"life\" + 0.003*\"bad\"'),\n",
       " (2,\n",
       "  '0.000*\"got\" + 0.000*\"addict\" + 0.000*\"think\" + 0.000*\"going\" + 0.000*\"feel\" + 0.000*\"makes\" + 0.000*\"life\" + 0.000*\"weed\" + 0.000*\"good\" + 0.000*\"drug\"'),\n",
       " (3,\n",
       "  '0.009*\"think\" + 0.009*\"weed\" + 0.008*\"makes\" + 0.007*\"drug\" + 0.007*\"try\" + 0.006*\"addict\" + 0.006*\"elliot\" + 0.005*\"going\" + 0.005*\"life\" + 0.005*\"got\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda for 4 topics\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY 2**  \n",
    "\n",
    "only nouns - `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>post_q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>smur2x</th>\n",
       "      <td>first congrats and continued success on your s...</td>\n",
       "      <td>Likely to try drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sn2vpk</th>\n",
       "      <td>i think the difference between elliot and rue ...</td>\n",
       "      <td>Elliots response to free drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqhl33</th>\n",
       "      <td>i already smoke weed and do psychedelics but i...</td>\n",
       "      <td>Realistic portrayal of withdrawal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body  \\\n",
       "post                                                        \n",
       "smur2x  first congrats and continued success on your s...   \n",
       "sn2vpk  i think the difference between elliot and rue ...   \n",
       "sqhl33  i already smoke weed and do psychedelics but i...   \n",
       "\n",
       "                                   post_q  \n",
       "post                                       \n",
       "smur2x                Likely to try drugs  \n",
       "sn2vpk     Elliots response to free drugs  \n",
       "sqhl33  Realistic portrayal of withdrawal  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read clean data\n",
    "data_clean = pd.read_pickle('../dat/corpus.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>smur2x</th>\n",
       "      <td>congrats success sobriety show exaddict downsi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sn2vpk</th>\n",
       "      <td>i difference elliot rue health issues life ell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqhl33</th>\n",
       "      <td>i weed psychedelics i opiates everything i add...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body\n",
       "post                                                     \n",
       "smur2x  congrats success sobriety show exaddict downsi...\n",
       "sn2vpk  i difference elliot rue health issues life ell...\n",
       "sqhl33  i weed psychedelics i opiates everything i add..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter so only nouns are left\n",
    "data_nouns = pd.DataFrame(data_clean.body.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aana</th>\n",
       "      <th>aback</th>\n",
       "      <th>abhorrence</th>\n",
       "      <th>ability</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutes</th>\n",
       "      <th>absorption</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abuser</th>\n",
       "      <th>...</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yum</th>\n",
       "      <th>yup</th>\n",
       "      <th>zendaya</th>\n",
       "      <th>zendayas</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>smur2x</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sn2vpk</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqhl33</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aana  aback  abhorrence  ability  absolute  absolutes  absorption  \\\n",
       "post                                                                            \n",
       "smur2x   1     0      0           0        0         2          0           1   \n",
       "sn2vpk   0     1      1           0        2         0          1           0   \n",
       "sqhl33   1     0      0           1        0         2          0           0   \n",
       "\n",
       "        abuse  abuser  ...  yuck  yum  yup  zendaya  zendayas  zombie  zone  \\\n",
       "post                   ...                                                    \n",
       "smur2x      0       0  ...     1    1    0        6         1       0     0   \n",
       "sn2vpk      4       0  ...     0    0    0        2         0       1     0   \n",
       "sqhl33      6       1  ...     0    0    1        4         0       1     1   \n",
       "\n",
       "        zoo  zooming  zs  \n",
       "post                      \n",
       "smur2x    0        1   1  \n",
       "sn2vpk    0        0   0  \n",
       "sqhl33    1        0   0  \n",
       "\n",
       "[3 rows x 2675 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new dtm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# add additional stop words\n",
    "add_stop_words = ['i', 'like','just','rue','did','really','people','way','know','use',\n",
    "                  'time','drugs','want','does','addiction']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# dtm\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvnn = cvn.fit_transform(data_nouns.body)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(sp.csr_matrix(data_dtmn.transpose()))\n",
    "# vocab\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"drug\" + 0.001*\"life\" + 0.001*\"weed\" + 0.001*\"episode\" + 0.001*\"opiates\" + 0.001*\"thing\" + 0.001*\"season\" + 0.001*\"pain\" + 0.001*\"jules\" + 0.001*\"day\"'),\n",
       " (1,\n",
       "  '0.015*\"drug\" + 0.012*\"addict\" + 0.012*\"life\" + 0.010*\"season\" + 0.010*\"weed\" + 0.009*\"episode\" + 0.009*\"opiates\" + 0.009*\"jules\" + 0.008*\"years\" + 0.007*\"lot\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 topics\n",
    "ldan = models.ldamodel.LdaModel(corpus=corpusn, id2word=id2wordn, num_topics=2, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"season\" + 0.014*\"life\" + 0.014*\"weed\" + 0.013*\"drug\" + 0.013*\"opiates\" + 0.011*\"addict\" + 0.011*\"euphoria\" + 0.010*\"episode\" + 0.010*\"pain\" + 0.009*\"years\"'),\n",
       " (1,\n",
       "  '0.001*\"drug\" + 0.001*\"life\" + 0.001*\"addict\" + 0.001*\"jules\" + 0.001*\"pain\" + 0.001*\"years\" + 0.001*\"episode\" + 0.001*\"thing\" + 0.001*\"season\" + 0.001*\"elliot\"'),\n",
       " (2,\n",
       "  '0.021*\"jules\" + 0.018*\"elliot\" + 0.016*\"drug\" + 0.015*\"addict\" + 0.009*\"plan\" + 0.008*\"heroin\" + 0.008*\"rues\" + 0.008*\"thing\" + 0.007*\"life\" + 0.007*\"laurie\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 topics\n",
    "ldan = models.ldamodel.LdaModel(corpus=corpusn, id2word=id2wordn, num_topics=3, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"jules\" + 0.019*\"elliot\" + 0.017*\"drug\" + 0.016*\"addict\" + 0.010*\"plan\" + 0.009*\"heroin\" + 0.009*\"rues\" + 0.008*\"thing\" + 0.008*\"life\" + 0.007*\"laurie\"'),\n",
       " (1,\n",
       "  '0.002*\"elliot\" + 0.002*\"drug\" + 0.001*\"addict\" + 0.001*\"jules\" + 0.001*\"life\" + 0.001*\"heroin\" + 0.001*\"thing\" + 0.001*\"rues\" + 0.001*\"episode\" + 0.001*\"things\"'),\n",
       " (2,\n",
       "  '0.015*\"season\" + 0.014*\"life\" + 0.014*\"weed\" + 0.014*\"drug\" + 0.013*\"opiates\" + 0.011*\"addict\" + 0.011*\"euphoria\" + 0.011*\"episode\" + 0.010*\"pain\" + 0.010*\"years\"'),\n",
       " (3,\n",
       "  '0.001*\"drug\" + 0.001*\"life\" + 0.001*\"pain\" + 0.001*\"weed\" + 0.001*\"episode\" + 0.001*\"addict\" + 0.001*\"opiates\" + 0.001*\"euphoria\" + 0.001*\"season\" + 0.001*\"things\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 topics\n",
    "ldan = models.ldamodel.LdaModel(corpus=corpusn, id2word=id2wordn, num_topics=4, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try 3 - nouns and adjectives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_adj(text):\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)]\n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>smur2x</th>\n",
       "      <td>congrats continued success sobriety incredible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sn2vpk</th>\n",
       "      <td>i difference elliot rue mental health issues m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqhl33</th>\n",
       "      <td>i weed psychedelics i opiates everything i add...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body\n",
       "post                                                     \n",
       "smur2x  congrats continued success sobriety incredible...\n",
       "sn2vpk  i difference elliot rue mental health issues m...\n",
       "sqhl33  i weed psychedelics i opiates everything i add..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj = pd.DataFrame(data_clean.body.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aana</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abby</th>\n",
       "      <th>abhorrence</th>\n",
       "      <th>ability</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutes</th>\n",
       "      <th>absorption</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yum</th>\n",
       "      <th>yup</th>\n",
       "      <th>zendayas</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>smur2x</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sn2vpk</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqhl33</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aana  ab  aback  abby  abhorrence  ability  absolute  absolutes  \\\n",
       "post                                                                          \n",
       "smur2x   1     0   0      0     1           0        0         4          0   \n",
       "sn2vpk   0     1   1      1     0           0        2         0          1   \n",
       "sqhl33   3     0   0      0     0           1        0         4          0   \n",
       "\n",
       "        absorption  ...  youtube  yuck  yum  yup  zendayas  zombie  zone  zoo  \\\n",
       "post                ...                                                         \n",
       "smur2x           1  ...        0     1    1    0         1       0     0    0   \n",
       "sn2vpk           0  ...        0     0    0    1         0       1     0    0   \n",
       "sqhl33           0  ...        1     0    0    1         0       1     1    1   \n",
       "\n",
       "        zooming  zs  \n",
       "post                 \n",
       "smur2x        1   1  \n",
       "sn2vpk        0   0  \n",
       "sqhl33        0   0  \n",
       "\n",
       "[3 rows x 3135 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tdm\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=0.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.body)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(sp.csr_matrix(data_dtmna.transpose()))\n",
    "# vocab\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"yawning\" + 0.005*\"realistic\" + 0.004*\"opioids\" + 0.004*\"wd\" + 0.004*\"cold\" + 0.003*\"dopamine\" + 0.002*\"portrayal\" + 0.002*\"track\" + 0.002*\"ep\" + 0.002*\"acting\"'),\n",
       " (1,\n",
       "  '0.017*\"weed\" + 0.012*\"elliot\" + 0.006*\"fun\" + 0.005*\"psychedelics\" + 0.005*\"idea\" + 0.004*\"kid\" + 0.004*\"shrooms\" + 0.004*\"addictive\" + 0.004*\"party\" + 0.004*\"interested\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 topics\n",
    "ldana = models.ldamodel.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=2, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"weed\" + 0.001*\"elliot\" + 0.000*\"psychedelics\" + 0.000*\"fun\" + 0.000*\"idea\" + 0.000*\"shrooms\" + 0.000*\"addictive\" + 0.000*\"opioids\" + 0.000*\"kid\" + 0.000*\"party\"'),\n",
       " (1,\n",
       "  '0.020*\"weed\" + 0.007*\"psychedelics\" + 0.005*\"shrooms\" + 0.005*\"fun\" + 0.005*\"opioids\" + 0.004*\"addictive\" + 0.004*\"interested\" + 0.003*\"party\" + 0.003*\"couple\" + 0.003*\"desire\"'),\n",
       " (2,\n",
       "  '0.028*\"elliot\" + 0.009*\"kid\" + 0.007*\"idea\" + 0.007*\"fez\" + 0.006*\"line\" + 0.005*\"fun\" + 0.005*\"hes\" + 0.005*\"social\" + 0.004*\"character\" + 0.004*\"moment\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 topics\n",
    "ldana = models.ldamodel.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=3, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.026*\"weed\" + 0.009*\"psychedelics\" + 0.007*\"shrooms\" + 0.006*\"fun\" + 0.006*\"addictive\" + 0.005*\"interested\" + 0.004*\"party\" + 0.004*\"opioids\" + 0.004*\"desire\" + 0.003*\"scary\"'),\n",
       " (1,\n",
       "  '0.001*\"weed\" + 0.001*\"elliot\" + 0.000*\"psychedelics\" + 0.000*\"fun\" + 0.000*\"idea\" + 0.000*\"party\" + 0.000*\"interested\" + 0.000*\"addictive\" + 0.000*\"shrooms\" + 0.000*\"opioids\"'),\n",
       " (2,\n",
       "  '0.031*\"elliot\" + 0.009*\"kid\" + 0.008*\"idea\" + 0.008*\"fez\" + 0.007*\"line\" + 0.006*\"fun\" + 0.006*\"hes\" + 0.005*\"social\" + 0.005*\"character\" + 0.004*\"moment\"'),\n",
       " (3,\n",
       "  '0.010*\"yawning\" + 0.007*\"realistic\" + 0.005*\"opioids\" + 0.005*\"cold\" + 0.005*\"wd\" + 0.004*\"dopamine\" + 0.003*\"portrayal\" + 0.003*\"track\" + 0.003*\"ep\" + 0.003*\"acting\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 topics\n",
    "ldana = models.ldamodel.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=4, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ID topics**\n",
    "\n",
    "last model makes the most sense - 4topics with nouns and adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.026*\"weed\" + 0.009*\"psychedelics\" + 0.007*\"shrooms\" + 0.006*\"fun\" + 0.006*\"addictive\" + 0.005*\"interested\" + 0.004*\"party\" + 0.004*\"opioids\" + 0.004*\"desire\" + 0.003*\"scary\"'),\n",
       " (1,\n",
       "  '0.000*\"bloodstream\" + 0.000*\"blessing\" + 0.000*\"shoot\" + 0.000*\"tough\" + 0.000*\"lie\" + 0.000*\"psychopath\" + 0.000*\"prevalent\" + 0.000*\"thread\" + 0.000*\"powerful\" + 0.000*\"bot\"'),\n",
       " (2,\n",
       "  '0.031*\"elliot\" + 0.010*\"kid\" + 0.008*\"idea\" + 0.008*\"fez\" + 0.007*\"line\" + 0.006*\"hes\" + 0.006*\"fun\" + 0.005*\"social\" + 0.005*\"character\" + 0.004*\"moment\"'),\n",
       " (3,\n",
       "  '0.010*\"yawning\" + 0.007*\"realistic\" + 0.005*\"cold\" + 0.005*\"wd\" + 0.005*\"opioids\" + 0.004*\"dopamine\" + 0.003*\"ep\" + 0.003*\"track\" + 0.003*\"portrayal\" + 0.003*\"incredible\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 topics - more passes\n",
    "ldana = models.ldamodel.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=4, passes=100)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topics**\n",
    "\n",
    "- topic 0: types of drugs, party\n",
    "- topic 1: drug actions (?)\n",
    "- topic 2: characters in the show\n",
    "- topic 3: effects of drugs (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'smur2x'), (2, 'sn2vpk'), (0, 'sqhl33')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the topic dist of the document\n",
    "\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make gensim Dictionary from the corpus\n",
    "dict_na = corpora.Dictionary([data_dtmna.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "coh_model_lda = models.CoherenceModel(model=ldana, \n",
    "                                      texts=data_dtmna.columns,\n",
    "                                      coherence='c_v',\n",
    "                                      dictionary=dict_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence score:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    }
   ],
   "source": [
    "coh_lda = coh_model_lda.get_coherence()\n",
    "print('coherence score: ', coh_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence analysis to find optimal number of topics\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "    start : Initial num of topics\n",
    "    step : Increment between each topic number\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'id2token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=0'>1</a>\u001b[0m compute_coherence_values(dictionary\u001b[39m=\u001b[39;49mid2wordna, corpus\u001b[39m=\u001b[39;49mcorpusna, texts\u001b[39m=\u001b[39;49mdata_nouns_adj\u001b[39m.\u001b[39;49mbody, start\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, limit\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m, step\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb Cell 36\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[0;34m(dictionary, corpus, texts, limit, start, step)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=23'>24</a>\u001b[0m     model_list\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=24'>25</a>\u001b[0m     coherencemodel \u001b[39m=\u001b[39m CoherenceModel(model\u001b[39m=\u001b[39mmodel, texts\u001b[39m=\u001b[39mtexts, dictionary\u001b[39m=\u001b[39mdictionary, coherence\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mc_v\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=25'>26</a>\u001b[0m     coherence_values\u001b[39m.\u001b[39mappend(coherencemodel\u001b[39m.\u001b[39;49mget_coherence())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model_list, coherence_values\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/coherencemodel.py:615\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_coherence\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    607\u001b[0m     \u001b[39m\"\"\"Get coherence value based on pipeline parameters.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m \n\u001b[1;32m    614\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m     confirmed_measures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_coherence_per_topic()\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_measures(confirmed_measures)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/coherencemodel.py:575\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    573\u001b[0m     segmented_topics \u001b[39m=\u001b[39m measure\u001b[39m.\u001b[39mseg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopics)\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accumulator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimate_probabilities(segmented_topics)\n\u001b[1;32m    577\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(with_std\u001b[39m=\u001b[39mwith_std, with_support\u001b[39m=\u001b[39mwith_support)\n\u001b[1;32m    578\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoherence \u001b[39min\u001b[39;00m BOOLEAN_DOCUMENT_BASED \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoherence \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mc_w2v\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/coherencemodel.py:547\u001b[0m, in \u001b[0;36mCoherenceModel.estimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoherence \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mc_w2v\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    545\u001b[0m         kwargs[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeyed_vectors\n\u001b[0;32m--> 547\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accumulator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeasure\u001b[39m.\u001b[39;49mprob(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    549\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accumulator\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/probability_estimation.py:154\u001b[0m, in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    152\u001b[0m     accumulator \u001b[39m=\u001b[39m WordOccurrenceAccumulator(top_ids, dictionary)\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     accumulator \u001b[39m=\u001b[39m ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n\u001b[1;32m    155\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39musing \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to estimate probabilities from sliding windows\u001b[39m\u001b[39m\"\u001b[39m, accumulator)\n\u001b[1;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m accumulator\u001b[39m.\u001b[39maccumulate(texts, window_size)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:424\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.__init__\u001b[0;34m(self, processes, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, processes, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 424\u001b[0m     \u001b[39msuper\u001b[39;49m(ParallelWordOccurrenceAccumulator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m processes \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    426\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust have at least 2 processes to run in parallel; got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m processes)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:287\u001b[0m, in \u001b[0;36mWindowedTextsAnalyzer.__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, relevant_ids, dictionary):\n\u001b[1;32m    277\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[1;32m    279\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39msuper\u001b[39;49m(WindowedTextsAnalyzer, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(relevant_ids, dictionary)\n\u001b[1;32m    288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_none_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab_size\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:190\u001b[0m, in \u001b[0;36mUsesDictionary.__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39msuper\u001b[39m(UsesDictionary, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(relevant_ids)\n\u001b[0;32m--> 190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelevant_words \u001b[39m=\u001b[39m _ids_to_words(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelevant_ids, dictionary)\n\u001b[1;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdictionary \u001b[39m=\u001b[39m dictionary\n\u001b[1;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken2id \u001b[39m=\u001b[39m dictionary\u001b[39m.\u001b[39mtoken2id\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:56\u001b[0m, in \u001b[0;36m_ids_to_words\u001b[0;34m(ids, dictionary)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ids_to_words\u001b[39m(ids, dictionary):\n\u001b[1;32m     26\u001b[0m     \u001b[39m\"\"\"Convert an iterable of ids to their corresponding words using a dictionary.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Abstract away the differences between the HashDictionary and the standard one.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dictionary\u001b[39m.\u001b[39;49mid2token:  \u001b[39m# may not be initialized in the standard gensim.corpora.Dictionary\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         \u001b[39msetattr\u001b[39m(dictionary, \u001b[39m'\u001b[39m\u001b[39mid2token\u001b[39m\u001b[39m'\u001b[39m, {v: k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m dictionary\u001b[39m.\u001b[39mtoken2id\u001b[39m.\u001b[39mitems()})\n\u001b[1;32m     59\u001b[0m     top_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'id2token'"
     ]
    }
   ],
   "source": [
    "compute_coherence_values(dictionary=id2wordna, corpus=corpusna, texts=data_nouns_adj.body, start=2, limit=40, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement LDA\n",
    "def lda_model(corpus, dictionary, num_topics=10, passes=20):\n",
    "    \"\"\"\n",
    "    Create LDA model\n",
    "    \"\"\"\n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                             id2word=dictionary,\n",
    "                             num_topics=num_topics,\n",
    "                             passes=passes,\n",
    "                             workers=2)\n",
    "    return lda_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
