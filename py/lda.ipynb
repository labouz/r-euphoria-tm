{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering impact of the Series 'Euphoria' through NLP\n",
    "### Analysis based on posts and comments on the `r/euphoria` subreddit  \n",
    "\n",
    "#### 3. LDA - Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= *Every documents is probability dist of topics*\n",
    "\n",
    "*goal*: LDA learns the topic mix in each doc, then words in each topic   \n",
    "\n",
    "*how*: LDA randomly assigns topics to words (will be wrong). Then, iterativly, looks for how often the topic occus in the doc and how often the word occurs in the topic overall. Based on this infor, assign the word a new topic.\n",
    "\n",
    "`k = 2` is a good starting part for number of topics  \n",
    "\n",
    "*input*: TDM, K, num iterations  \n",
    "*output*: top words in each topic - figure out if they make sense\n",
    "\n",
    "*tools*:  \n",
    "`gensim`\n",
    "\n",
    "alternate factorization methods: \n",
    "- NMF\n",
    "- LSI\n",
    "- look at BERT TM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all posts in the r/euphoria subreddit that match 'rue' and are between jan9 and feb 2 2022 for season 2 \n",
    "\n",
    "**1st experiment**\n",
    "N ~ 20,823 post + comments (100%)\n",
    "Some notable comments:  \n",
    "- \"I was curious, so I read the /r/opiates discussion on the episode and they agree it's accurate. There's not a single negative comment.\"\n",
    "- \"Having been the partner/girlfriend of an addict, I cannot re-watch several parts of Season 2 because they feel so real to me.\"\n",
    "- \"Absolutely spot on. I've had my struggles w Vicodin and there were points when I was dope sick, I would do anything to get those damn pills. I'm still feeling some type of a way after last night's episode. That's how accurate it was\"\n",
    "- \"Yâ€™all attach too much of your selves to the characters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXP 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaedefabdfef</th>\n",
       "      <th>abafbfbedbada</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abandons</th>\n",
       "      <th>abashed</th>\n",
       "      <th>...</th>\n",
       "      <th>zorbcaps</th>\n",
       "      <th>zoted</th>\n",
       "      <th>zouabi</th>\n",
       "      <th>zoya</th>\n",
       "      <th>zqcsrpwsge</th>\n",
       "      <th>zrue</th>\n",
       "      <th>zs</th>\n",
       "      <th>zsuzsana</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19282</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19196 rows Ã— 25532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ab  aback  abaedefabdfef  abafbfbedbada  abandon  abandoned  \\\n",
       "0       0      0              0              0        0          0   \n",
       "1       0      0              0              0        0          0   \n",
       "2       0      0              0              0        0          0   \n",
       "3       0      0              0              0        0          0   \n",
       "4       0      0              0              0        0          0   \n",
       "...    ..    ...            ...            ...      ...        ...   \n",
       "19278   0      0              0              0        0          0   \n",
       "19279   0      0              0              0        0          0   \n",
       "19280   0      0              0              0        0          0   \n",
       "19281   0      0              0              0        0          0   \n",
       "19282   0      0              0              0        0          0   \n",
       "\n",
       "       abandoning  abandonment  abandons  abashed  ...  zorbcaps  zoted  \\\n",
       "0               0            0         0        0  ...         0      0   \n",
       "1               0            0         0        0  ...         0      0   \n",
       "2               0            0         0        0  ...         0      0   \n",
       "3               0            0         0        0  ...         0      0   \n",
       "4               0            0         0        0  ...         0      0   \n",
       "...           ...          ...       ...      ...  ...       ...    ...   \n",
       "19278           0            0         0        0  ...         0      0   \n",
       "19279           0            0         0        0  ...         0      0   \n",
       "19280           0            0         0        0  ...         0      0   \n",
       "19281           0            0         0        0  ...         0      0   \n",
       "19282           0            0         0        0  ...         0      0   \n",
       "\n",
       "       zouabi  zoya  zqcsrpwsge  zrue  zs  zsuzsana  zurich  zy  \n",
       "0           0     0           0     0   0         0       0   0  \n",
       "1           0     0           0     0   0         0       0   0  \n",
       "2           0     0           0     0   0         0       0   0  \n",
       "3           0     0           0     0   0         0       0   0  \n",
       "4           0     0           0     0   0         0       0   0  \n",
       "...       ...   ...         ...   ...  ..       ...     ...  ..  \n",
       "19278       0     0           0     0   0         0       0   0  \n",
       "19279       0     0           0     0   0         0       0   0  \n",
       "19280       0     0           0     0   0         0       0   0  \n",
       "19281       0     0           0     0   0         0       0   0  \n",
       "19282       0     0           0     0   0         0       0   0  \n",
       "\n",
       "[19196 rows x 25532 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bring in data\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('../dat/tdm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19272</th>\n",
       "      <th>19273</th>\n",
       "      <th>19274</th>\n",
       "      <th>19275</th>\n",
       "      <th>19277</th>\n",
       "      <th>19278</th>\n",
       "      <th>19279</th>\n",
       "      <th>19280</th>\n",
       "      <th>19281</th>\n",
       "      <th>19282</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abaedefabdfef</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abafbfbedbada</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      1      2      3      4      5      6      7      8      \\\n",
       "ab                 0      0      0      0      0      0      0      0      0   \n",
       "aback              0      0      0      0      0      0      0      0      0   \n",
       "abaedefabdfef      0      0      0      0      0      0      0      0      0   \n",
       "abafbfbedbada      0      0      0      0      0      0      0      0      0   \n",
       "abandon            0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "               9      ...  19272  19273  19274  19275  19277  19278  19279  \\\n",
       "ab                 0  ...      0      0      0      0      0      0      0   \n",
       "aback              0  ...      0      0      0      0      0      0      0   \n",
       "abaedefabdfef      0  ...      0      0      0      0      0      0      0   \n",
       "abafbfbedbada      0  ...      0      0      0      0      0      0      0   \n",
       "abandon            0  ...      0      0      0      0      0      0      0   \n",
       "\n",
       "               19280  19281  19282  \n",
       "ab                 0      0      0  \n",
       "aback              0      0      0  \n",
       "abaedefabdfef      0      0      0  \n",
       "abafbfbedbada      0      0      0  \n",
       "abandon            0      0      0  \n",
       "\n",
       "[5 rows x 19196 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put tdm in gensim format\n",
    "sparse_counts = sp.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary of all terms - required by gensim\n",
    "# cv contains the whole vocabulary of the corpus\n",
    "cv = pickle.load(open('../dat/cv_Stop.pkl', 'rb'))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have corpus and id2word, now we can create the lda model\n",
    "# specify other parameters\n",
    "# more passes, more it may make sense\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"jules\" + 0.018*\"like\" + 0.014*\"think\" + 0.010*\"ml\" + 0.009*\"season\" + 0.008*\"going\" + 0.008*\"box\" + 0.008*\"nate\" + 0.008*\"know\" + 0.008*\"really\"'),\n",
       " (1,\n",
       "  '0.007*\"people\" + 0.006*\"amp\" + 0.004*\"like\" + 0.004*\"addiction\" + 0.004*\"time\" + 0.003*\"xb\" + 0.003*\"life\" + 0.003*\"day\" + 0.003*\"wel\" + 0.003*\"know\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.026*\"jules\" + 0.022*\"like\" + 0.015*\"think\" + 0.012*\"people\" + 0.011*\"nate\" + 0.011*\"season\" + 0.009*\"really\" + 0.008*\"cassie\" + 0.008*\"does\" + 0.007*\"know\"'),\n",
       " (1,\n",
       "  '0.012*\"fez\" + 0.010*\"like\" + 0.009*\"going\" + 0.009*\"know\" + 0.008*\"did\" + 0.007*\"think\" + 0.007*\"time\" + 0.006*\"drugs\" + 0.005*\"episode\" + 0.005*\"got\"'),\n",
       " (2,\n",
       "  '0.024*\"ml\" + 0.019*\"box\" + 0.015*\"niche\" + 0.012*\"tester\" + 0.010*\"amp\" + 0.007*\"designer\" + 0.006*\"new\" + 0.006*\"vintage\" + 0.005*\"cap\" + 0.005*\"xb\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda for 3 topics\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.034*\"jules\" + 0.021*\"like\" + 0.017*\"think\" + 0.014*\"people\" + 0.010*\"does\" + 0.009*\"really\" + 0.009*\"elliot\" + 0.008*\"know\" + 0.008*\"feel\" + 0.007*\"relationship\"'),\n",
       " (1,\n",
       "  '0.027*\"ml\" + 0.021*\"box\" + 0.017*\"niche\" + 0.014*\"tester\" + 0.011*\"amp\" + 0.008*\"designer\" + 0.007*\"vintage\" + 0.007*\"new\" + 0.006*\"cap\" + 0.006*\"xb\"'),\n",
       " (2,\n",
       "  '0.020*\"season\" + 0.018*\"like\" + 0.013*\"nate\" + 0.011*\"cassie\" + 0.011*\"episode\" + 0.010*\"lexi\" + 0.010*\"character\" + 0.010*\"think\" + 0.008*\"characters\" + 0.008*\"maddy\"'),\n",
       " (3,\n",
       "  '0.012*\"fez\" + 0.008*\"going\" + 0.007*\"did\" + 0.007*\"time\" + 0.007*\"know\" + 0.006*\"like\" + 0.006*\"drugs\" + 0.005*\"drug\" + 0.005*\"got\" + 0.005*\"think\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda for 4 topics\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY 2**  \n",
    "\n",
    "only nouns - `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it is a turning point for the series fs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am glad we finally get a whole episode dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my thoughts exactly all i could think of while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i honestly thought with the end of ep the seas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>same i feel like we have not really seen much ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19278</th>\n",
       "      <td>the which extremely out franchised culver in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19279</th>\n",
       "      <td>so this hapened back in late november and i on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>i want to use the face claim of alexis bledel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19281</th>\n",
       "      <td>i just ordered some seeds and i am wanting to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19282</th>\n",
       "      <td>i watched the bridge episode but what was the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19196 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0                it is a turning point for the series fs\n",
       "1      i am glad we finally get a whole episode dedic...\n",
       "2      my thoughts exactly all i could think of while...\n",
       "3      i honestly thought with the end of ep the seas...\n",
       "4      same i feel like we have not really seen much ...\n",
       "...                                                  ...\n",
       "19278  the which extremely out franchised culver in t...\n",
       "19279  so this hapened back in late november and i on...\n",
       "19280  i want to use the face claim of alexis bledel ...\n",
       "19281  i just ordered some seeds and i am wanting to ...\n",
       "19282  i watched the bridge episode but what was the ...\n",
       "\n",
       "[19196 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read clean data\n",
    "data_clean = pd.read_pickle('../dat/corpus.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>point series fs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i glad episode protagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thoughts i watching show shitshow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i end ep season space kind mediocre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>season episode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19278</th>\n",
       "      <td>culver family wel desserts etc culvers culvers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19279</th>\n",
       "      <td>november i work dimsum restaurant city folowin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>i face claim bledel character specificaly seas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19281</th>\n",
       "      <td>i seeds way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19282</th>\n",
       "      <td>i bridge episode timeline jules rue i detail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19196 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0                                        point series fs\n",
       "1                             i glad episode protagonist\n",
       "2                      thoughts i watching show shitshow\n",
       "3                    i end ep season space kind mediocre\n",
       "4                                         season episode\n",
       "...                                                  ...\n",
       "19278  culver family wel desserts etc culvers culvers...\n",
       "19279  november i work dimsum restaurant city folowin...\n",
       "19280  i face claim bledel character specificaly seas...\n",
       "19281                                        i seeds way\n",
       "19282       i bridge episode timeline jules rue i detail\n",
       "\n",
       "[19196 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter so only nouns are left\n",
    "data_nouns = pd.DataFrame(data_clean[0].apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abandons</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abducts</th>\n",
       "      <th>abe</th>\n",
       "      <th>abey</th>\n",
       "      <th>abfafebfbad</th>\n",
       "      <th>abfbmltmqspf</th>\n",
       "      <th>...</th>\n",
       "      <th>zomer</th>\n",
       "      <th>zomg</th>\n",
       "      <th>zone</th>\n",
       "      <th>zongao</th>\n",
       "      <th>zora</th>\n",
       "      <th>zorbcaps</th>\n",
       "      <th>zouabi</th>\n",
       "      <th>zoya</th>\n",
       "      <th>zrue</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19282</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19196 rows Ã— 16175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aback  abandon  abandonment  abandons  abdomen  abducts  abe  abey  \\\n",
       "0          0        0            0         0        0        0    0     0   \n",
       "1          0        0            0         0        0        0    0     0   \n",
       "2          0        0            0         0        0        0    0     0   \n",
       "3          0        0            0         0        0        0    0     0   \n",
       "4          0        0            0         0        0        0    0     0   \n",
       "...      ...      ...          ...       ...      ...      ...  ...   ...   \n",
       "19278      0        0            0         0        0        0    0     0   \n",
       "19279      0        0            0         0        0        0    0     0   \n",
       "19280      0        0            0         0        0        0    0     0   \n",
       "19281      0        0            0         0        0        0    0     0   \n",
       "19282      0        0            0         0        0        0    0     0   \n",
       "\n",
       "       abfafebfbad  abfbmltmqspf  ...  zomer  zomg  zone  zongao  zora  \\\n",
       "0                0             0  ...      0     0     0       0     0   \n",
       "1                0             0  ...      0     0     0       0     0   \n",
       "2                0             0  ...      0     0     0       0     0   \n",
       "3                0             0  ...      0     0     0       0     0   \n",
       "4                0             0  ...      0     0     0       0     0   \n",
       "...            ...           ...  ...    ...   ...   ...     ...   ...   \n",
       "19278            0             0  ...      0     0     0       0     0   \n",
       "19279            0             0  ...      0     0     0       0     0   \n",
       "19280            0             0  ...      0     0     0       0     0   \n",
       "19281            0             0  ...      0     0     0       0     0   \n",
       "19282            0             0  ...      0     0     0       0     0   \n",
       "\n",
       "       zorbcaps  zouabi  zoya  zrue  zurich  \n",
       "0             0       0     0     0       0  \n",
       "1             0       0     0     0       0  \n",
       "2             0       0     0     0       0  \n",
       "3             0       0     0     0       0  \n",
       "4             0       0     0     0       0  \n",
       "...         ...     ...   ...   ...     ...  \n",
       "19278         0       0     0     0       0  \n",
       "19279         0       0     0     0       0  \n",
       "19280         0       0     0     0       0  \n",
       "19281         0       0     0     0       0  \n",
       "19282         0       0     0     0       0  \n",
       "\n",
       "[19196 rows x 16175 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new dtm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# add additional stop words - HOW TO ACCOUNT FOR PEOPLE BANGING ON KEYBOARDS?!?!?!?\n",
    "add_stop_words = ['i', 'just','did', 'ab', 'abc', 'abcb', 'abcny', 'abd', 'abdabca', 'fs', \n",
    "                  'zpqxhxhzanapjsjbf', 'zqcsrpwsge', 'zqnuhckwdqwrhkuo', 'zs', 'zshwbhethehenozxfyqg',\n",
    "                  'zsmkbrmwngzsibrntkt', 'zy', 'zwhnrmujykdxmntiub', 'afqjcnguytghbsuvixmglpwzqbg', 'ebecadcbdfcbafbdb',\n",
    "                  'abfbmltmqspf', 'abfafebfbad']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# dtm\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns[0])\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(sp.csr_matrix(data_dtmn.transpose()))\n",
    "# vocab\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 topics\n",
    "ldan = models.ldamodel.LdaModel(corpus=corpusn, id2word=id2wordn, num_topics=2, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 topics\n",
    "ldan = models.ldamodel.LdaModel(corpus=corpusn, id2word=id2wordn, num_topics=3, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.030*\"box\" + 0.022*\"niche\" + 0.020*\"tester\" + 0.011*\"designer\" + 0.010*\"vintage\" + 0.009*\"amp\" + 0.008*\"cap\" + 0.006*\"chanel\" + 0.006*\"xb\" + 0.004*\"city\"'),\n",
       " (1,\n",
       "  '0.091*\"rue\" + 0.089*\"jules\" + 0.023*\"episode\" + 0.019*\"relationship\" + 0.015*\"elliot\" + 0.015*\"way\" + 0.013*\"people\" + 0.012*\"drugs\" + 0.011*\"rues\" + 0.010*\"season\"'),\n",
       " (2,\n",
       "  '0.019*\"drugs\" + 0.017*\"people\" + 0.016*\"fez\" + 0.015*\"rue\" + 0.015*\"drug\" + 0.015*\"time\" + 0.015*\"life\" + 0.009*\"way\" + 0.008*\"family\" + 0.007*\"house\"'),\n",
       " (3,\n",
       "  '0.040*\"season\" + 0.027*\"cassie\" + 0.026*\"nate\" + 0.026*\"character\" + 0.024*\"people\" + 0.022*\"characters\" + 0.015*\"maddy\" + 0.015*\"lexi\" + 0.013*\"lot\" + 0.012*\"story\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 topics\n",
    "ldan = models.ldamodel.LdaModel(corpus=corpusn, id2word=id2wordn, num_topics=4, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try 3 - nouns and adjectives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_adj(text):\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)]\n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>turning point series fs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i glad whole episode protagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thoughts i watching show shitshow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i end ep season space kind mediocre real narative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>same i season whole episode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19278</th>\n",
       "      <td>culver famous branched restaurant family wel d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19279</th>\n",
       "      <td>late november i most i work nice chinese ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>i face claim alexis bledel character specifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19281</th>\n",
       "      <td>i seeds best way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19282</th>\n",
       "      <td>i bridge episode timeline jules rue i detail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19196 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0                                turning point series fs\n",
       "1                       i glad whole episode protagonist\n",
       "2                      thoughts i watching show shitshow\n",
       "3      i end ep season space kind mediocre real narative\n",
       "4                            same i season whole episode\n",
       "...                                                  ...\n",
       "19278  culver famous branched restaurant family wel d...\n",
       "19279  late november i most i work nice chinese ameri...\n",
       "19280  i face claim alexis bledel character specifica...\n",
       "19281                                   i seeds best way\n",
       "19282       i bridge episode timeline jules rue i detail\n",
       "\n",
       "[19196 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj = pd.DataFrame(data_clean[0].apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abandons</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcb</th>\n",
       "      <th>abcny</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdabca</th>\n",
       "      <th>...</th>\n",
       "      <th>zpqxhxhzanapjsjbf</th>\n",
       "      <th>zqcsrpwsge</th>\n",
       "      <th>zqnuhckwdqwrhkuo</th>\n",
       "      <th>zrue</th>\n",
       "      <th>zs</th>\n",
       "      <th>zshwbhethehenozxfyqg</th>\n",
       "      <th>zsmkbrmwngzsibrntkt</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zwhnrmujykdxmntiub</th>\n",
       "      <th>zy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19286</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19287</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19288</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19289</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19290</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19288 rows Ã— 21389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ab  aback  abandon  abandonment  abandons  abc  abcb  abcny  abd  \\\n",
       "0       0      0        0            0         0    0     0      0    0   \n",
       "1       0      0        0            0         0    0     0      0    0   \n",
       "2       0      0        0            0         0    0     0      0    0   \n",
       "3       0      0        0            0         0    0     0      0    0   \n",
       "4       0      0        0            0         0    0     0      0    0   \n",
       "...    ..    ...      ...          ...       ...  ...   ...    ...  ...   \n",
       "19286   0      0        0            0         0    0     0      0    0   \n",
       "19287   0      0        0            0         0    0     0      0    0   \n",
       "19288   0      0        0            0         0    0     0      0    0   \n",
       "19289   0      0        0            0         0    0     0      0    0   \n",
       "19290   0      0        0            0         0    0     0      0    0   \n",
       "\n",
       "       abdabca  ...  zpqxhxhzanapjsjbf  zqcsrpwsge  zqnuhckwdqwrhkuo  zrue  \\\n",
       "0            0  ...                  0           0                 0     0   \n",
       "1            0  ...                  0           0                 0     0   \n",
       "2            0  ...                  0           0                 0     0   \n",
       "3            0  ...                  0           0                 0     0   \n",
       "4            0  ...                  0           0                 0     0   \n",
       "...        ...  ...                ...         ...               ...   ...   \n",
       "19286        0  ...                  0           0                 0     0   \n",
       "19287        0  ...                  0           0                 0     0   \n",
       "19288        0  ...                  0           0                 0     0   \n",
       "19289        0  ...                  0           0                 0     0   \n",
       "19290        0  ...                  0           0                 0     0   \n",
       "\n",
       "       zs  zshwbhethehenozxfyqg  zsmkbrmwngzsibrntkt  zurich  \\\n",
       "0       0                     0                    0       0   \n",
       "1       0                     0                    0       0   \n",
       "2       0                     0                    0       0   \n",
       "3       0                     0                    0       0   \n",
       "4       0                     0                    0       0   \n",
       "...    ..                   ...                  ...     ...   \n",
       "19286   0                     0                    0       0   \n",
       "19287   0                     0                    0       0   \n",
       "19288   0                     0                    0       0   \n",
       "19289   0                     0                    0       0   \n",
       "19290   0                     0                    0       0   \n",
       "\n",
       "       zwhnrmujykdxmntiub  zy  \n",
       "0                       0   0  \n",
       "1                       0   0  \n",
       "2                       0   0  \n",
       "3                       0   0  \n",
       "4                       0   0  \n",
       "...                   ...  ..  \n",
       "19286                   0   0  \n",
       "19287                   0   0  \n",
       "19288                   0   0  \n",
       "19289                   0   0  \n",
       "19290                   0   0  \n",
       "\n",
       "[19288 rows x 21389 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tdm\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=0.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj[0])\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(sp.csr_matrix(data_dtmna.transpose()))\n",
    "# vocab\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.036*\"rue\" + 0.025*\"jules\" + 0.014*\"people\" + 0.012*\"season\" + 0.010*\"nate\" + 0.009*\"episode\" + 0.008*\"drugs\" + 0.008*\"way\" + 0.007*\"time\" + 0.007*\"cassie\"'),\n",
       " (1,\n",
       "  '0.031*\"ful\" + 0.031*\"movie\" + 0.018*\"movies\" + 0.013*\"free\" + 0.012*\"online\" + 0.009*\"amp\" + 0.009*\"man\" + 0.008*\"home\" + 0.008*\"watch\" + 0.007*\"scream\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 topics\n",
    "ldana = models.ldamodel.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=2, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.052*\"rue\" + 0.036*\"jules\" + 0.016*\"people\" + 0.015*\"season\" + 0.014*\"nate\" + 0.013*\"episode\" + 0.011*\"drugs\" + 0.011*\"cassie\" + 0.010*\"way\" + 0.008*\"character\"'),\n",
       " (1,\n",
       "  '0.008*\"fez\" + 0.008*\"time\" + 0.006*\"school\" + 0.006*\"people\" + 0.006*\"season\" + 0.005*\"euphoria\" + 0.004*\"sam\" + 0.004*\"high\" + 0.004*\"year\" + 0.004*\"wel\"'),\n",
       " (2,\n",
       "  '0.033*\"ful\" + 0.033*\"movie\" + 0.019*\"movies\" + 0.014*\"free\" + 0.012*\"online\" + 0.010*\"amp\" + 0.009*\"man\" + 0.009*\"home\" + 0.008*\"watch\" + 0.008*\"scream\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 topics\n",
    "ldana = models.ldamodel.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=3, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.041*\"rue\" + 0.028*\"jules\" + 0.016*\"people\" + 0.013*\"season\" + 0.011*\"nate\" + 0.010*\"episode\" + 0.009*\"drugs\" + 0.008*\"time\" + 0.008*\"cassie\" + 0.008*\"way\"'),\n",
       " (1,\n",
       "  '0.023*\"king\" + 0.021*\"hero\" + 0.020*\"video\" + 0.012*\"academia\" + 0.011*\"heroes\" + 0.009*\"sound\" + 0.009*\"site\" + 0.008*\"tv\" + 0.007*\"films\" + 0.007*\"good\"'),\n",
       " (2,\n",
       "  '0.044*\"movie\" + 0.029*\"ful\" + 0.027*\"movies\" + 0.018*\"free\" + 0.017*\"online\" + 0.011*\"watch\" + 0.011*\"scream\" + 0.007*\"resolution\" + 0.007*\"hd\" + 0.007*\"quality\"'),\n",
       " (3,\n",
       "  '0.049*\"ful\" + 0.039*\"man\" + 0.030*\"ml\" + 0.029*\"home\" + 0.029*\"xb\" + 0.028*\"box\" + 0.026*\"way\" + 0.024*\"spider\" + 0.022*\"amp\" + 0.021*\"niche\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 topics\n",
    "ldana = models.ldamodel.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=4, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ID topics**\n",
    "\n",
    "last model makes the most sense - 4topics with nouns and adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.063*\"rue\" + 0.040*\"jules\" + 0.020*\"people\" + 0.016*\"drugs\" + 0.011*\"way\" + 0.011*\"elliot\" + 0.009*\"drug\" + 0.009*\"relationship\" + 0.008*\"life\" + 0.008*\"time\"'),\n",
       " (1,\n",
       "  '0.024*\"season\" + 0.021*\"nate\" + 0.016*\"cassie\" + 0.015*\"character\" + 0.012*\"episode\" + 0.012*\"characters\" + 0.012*\"fez\" + 0.010*\"lexi\" + 0.010*\"maddy\" + 0.010*\"jules\"'),\n",
       " (2,\n",
       "  '0.028*\"ful\" + 0.022*\"ml\" + 0.020*\"box\" + 0.015*\"niche\" + 0.013*\"tester\" + 0.010*\"amp\" + 0.008*\"designer\" + 0.007*\"new\" + 0.007*\"vintage\" + 0.006*\"xb\"'),\n",
       " (3,\n",
       "  '0.042*\"movie\" + 0.030*\"ful\" + 0.025*\"movies\" + 0.017*\"free\" + 0.016*\"online\" + 0.011*\"man\" + 0.010*\"watch\" + 0.010*\"home\" + 0.010*\"scream\" + 0.009*\"way\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 topics - more passes\n",
    "ldana = models.ldamodel.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=4, passes=100)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topics**\n",
    "\n",
    "- topic 0: types of drugs, party\n",
    "- topic 1: drug actions (?)\n",
    "- topic 2: characters in the show\n",
    "- topic 3: effects of drugs (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'smur2x'), (2, 'sn2vpk'), (0, 'sqhl33')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the topic dist of the document\n",
    "\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make gensim Dictionary from the corpus\n",
    "dict_na = corpora.Dictionary([data_dtmna.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "coh_model_lda = models.CoherenceModel(model=ldana, \n",
    "                                      texts=data_dtmna.columns,\n",
    "                                      coherence='c_v',\n",
    "                                      dictionary=dict_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence score:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    }
   ],
   "source": [
    "coh_lda = coh_model_lda.get_coherence()\n",
    "print('coherence score: ', coh_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence analysis to find optimal number of topics\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "    start : Initial num of topics\n",
    "    step : Increment between each topic number\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'id2token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=0'>1</a>\u001b[0m compute_coherence_values(dictionary\u001b[39m=\u001b[39;49mid2wordna, corpus\u001b[39m=\u001b[39;49mcorpusna, texts\u001b[39m=\u001b[39;49mdata_nouns_adj\u001b[39m.\u001b[39;49mbody, start\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, limit\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m, step\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb Cell 36\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[0;34m(dictionary, corpus, texts, limit, start, step)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=23'>24</a>\u001b[0m     model_list\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=24'>25</a>\u001b[0m     coherencemodel \u001b[39m=\u001b[39m CoherenceModel(model\u001b[39m=\u001b[39mmodel, texts\u001b[39m=\u001b[39mtexts, dictionary\u001b[39m=\u001b[39mdictionary, coherence\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mc_v\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=25'>26</a>\u001b[0m     coherence_values\u001b[39m.\u001b[39mappend(coherencemodel\u001b[39m.\u001b[39;49mget_coherence())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/fes/py/lda.ipynb#ch0000039?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model_list, coherence_values\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/coherencemodel.py:615\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_coherence\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    607\u001b[0m     \u001b[39m\"\"\"Get coherence value based on pipeline parameters.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m \n\u001b[1;32m    614\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m     confirmed_measures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_coherence_per_topic()\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_measures(confirmed_measures)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/coherencemodel.py:575\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    573\u001b[0m     segmented_topics \u001b[39m=\u001b[39m measure\u001b[39m.\u001b[39mseg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopics)\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accumulator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimate_probabilities(segmented_topics)\n\u001b[1;32m    577\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(with_std\u001b[39m=\u001b[39mwith_std, with_support\u001b[39m=\u001b[39mwith_support)\n\u001b[1;32m    578\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoherence \u001b[39min\u001b[39;00m BOOLEAN_DOCUMENT_BASED \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoherence \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mc_w2v\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/coherencemodel.py:547\u001b[0m, in \u001b[0;36mCoherenceModel.estimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoherence \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mc_w2v\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    545\u001b[0m         kwargs[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeyed_vectors\n\u001b[0;32m--> 547\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accumulator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeasure\u001b[39m.\u001b[39;49mprob(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    549\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accumulator\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/probability_estimation.py:154\u001b[0m, in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    152\u001b[0m     accumulator \u001b[39m=\u001b[39m WordOccurrenceAccumulator(top_ids, dictionary)\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     accumulator \u001b[39m=\u001b[39m ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n\u001b[1;32m    155\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39musing \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to estimate probabilities from sliding windows\u001b[39m\u001b[39m\"\u001b[39m, accumulator)\n\u001b[1;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m accumulator\u001b[39m.\u001b[39maccumulate(texts, window_size)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:424\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.__init__\u001b[0;34m(self, processes, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, processes, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 424\u001b[0m     \u001b[39msuper\u001b[39;49m(ParallelWordOccurrenceAccumulator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m processes \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    426\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust have at least 2 processes to run in parallel; got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m processes)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:287\u001b[0m, in \u001b[0;36mWindowedTextsAnalyzer.__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, relevant_ids, dictionary):\n\u001b[1;32m    277\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[1;32m    279\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39msuper\u001b[39;49m(WindowedTextsAnalyzer, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(relevant_ids, dictionary)\n\u001b[1;32m    288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_none_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab_size\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:190\u001b[0m, in \u001b[0;36mUsesDictionary.__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39msuper\u001b[39m(UsesDictionary, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(relevant_ids)\n\u001b[0;32m--> 190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelevant_words \u001b[39m=\u001b[39m _ids_to_words(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelevant_ids, dictionary)\n\u001b[1;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdictionary \u001b[39m=\u001b[39m dictionary\n\u001b[1;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken2id \u001b[39m=\u001b[39m dictionary\u001b[39m.\u001b[39mtoken2id\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:56\u001b[0m, in \u001b[0;36m_ids_to_words\u001b[0;34m(ids, dictionary)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ids_to_words\u001b[39m(ids, dictionary):\n\u001b[1;32m     26\u001b[0m     \u001b[39m\"\"\"Convert an iterable of ids to their corresponding words using a dictionary.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Abstract away the differences between the HashDictionary and the standard one.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dictionary\u001b[39m.\u001b[39;49mid2token:  \u001b[39m# may not be initialized in the standard gensim.corpora.Dictionary\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         \u001b[39msetattr\u001b[39m(dictionary, \u001b[39m'\u001b[39m\u001b[39mid2token\u001b[39m\u001b[39m'\u001b[39m, {v: k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m dictionary\u001b[39m.\u001b[39mtoken2id\u001b[39m.\u001b[39mitems()})\n\u001b[1;32m     59\u001b[0m     top_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'id2token'"
     ]
    }
   ],
   "source": [
    "compute_coherence_values(dictionary=id2wordna, corpus=corpusna, texts=data_nouns_adj.body, start=2, limit=40, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement LDA\n",
    "def lda_model(corpus, dictionary, num_topics=10, passes=20):\n",
    "    \"\"\"\n",
    "    Create LDA model\n",
    "    \"\"\"\n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                             id2word=dictionary,\n",
    "                             num_topics=num_topics,\n",
    "                             passes=passes,\n",
    "                             workers=2)\n",
    "    return lda_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
